{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6575b107",
   "metadata": {},
   "source": [
    "# TFM | Reinforcement Learning | Daniel Zorrilla | Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0745f64",
   "metadata": {},
   "source": [
    "## Installing additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aee1d",
   "metadata": {},
   "source": [
    "###### Installing stable baselines and pyglet library for developing games and other visually-rich applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561e7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58854c-77f5-4004-b9e8-a905ef61a7bc",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c101b-20bf-4dfe-9993-23ae40b186c3",
   "metadata": {},
   "source": [
    "## 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6267c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # provides a way of using OS dependent functionality. (files)\n",
    "import gym # Open AI gym\n",
    "from stable_baselines3 import PPO #PPO RL Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Creates a simple vectorized wrapper for multiple environments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # Test how well a model is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3670b76",
   "metadata": {},
   "source": [
    "## 2. Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1625fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Acrobot-v1' # Naming the Acrobot environment\n",
    "env = gym.make(environment_name) # Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-500.0\n",
      "Episode:2 Score:-500.0\n",
      "Episode:3 Score:-500.0\n",
      "Episode:4 Score:-500.0\n",
      "Episode:5 Score:-500.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5  # Number of episodes\n",
    "for episode in range (1, episodes+1): # Resetting environment  \n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # While episode active\n",
    "        env.render() # Visualizing environment\n",
    "        action = env.action_space.sample() # Creating sample actions\n",
    "        n_state, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward # Getting score\n",
    "    print('Episode:{} Score:{}'.format(episode,score)) # Printing episode and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c331912",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # Closing the render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c237420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # Understanding the action space of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d11c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Action random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92fef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # Understanding the observation space of this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db8c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1180349,  0.7808099,  0.6646522,  0.7259256,  0.8066243,\n",
       "       19.631912 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7efa8e",
   "metadata": {},
   "source": [
    "## 3. Train and create RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b6c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Acrobot') #where it is saved the tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e57a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Acrobot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e153fa",
   "metadata": {},
   "source": [
    "#### Install Pytorch *conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffe8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name) # Create environment\n",
    "env = DummyVecEnv([lambda: env]) # Wrapped environment using DummyVecEnv\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path) # Creating PPO Algorithm with MultiLayerPerceptron Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6611270",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 523  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 692        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00898079 |\n",
      "|    clip_fraction        | 0.0606     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.0111     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21         |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0082    |\n",
      "|    value_loss           | 147        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0073353 |\n",
      "|    clip_fraction        | 0.0376    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.08     |\n",
      "|    explained_variance   | 0.0884    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 21.4      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.00512  |\n",
      "|    value_loss           | 109       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009392764 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.0583      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    value_loss           | 95.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009374045 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00703    |\n",
      "|    value_loss           | 84.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 853          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077115083 |\n",
      "|    clip_fraction        | 0.0828       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.977       |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.9         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004559269 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    value_loss           | 89.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053267824 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.918       |\n",
      "|    explained_variance   | 0.407        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    value_loss           | 69.2         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 893        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00809076 |\n",
      "|    clip_fraction        | 0.0894     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.909     |\n",
      "|    explained_variance   | 0.347      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.1       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 898         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008814175 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.856      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    value_loss           | 54.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 909         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006453652 |\n",
      "|    clip_fraction        | 0.0547      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045686155 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00479     |\n",
      "|    value_loss           | 41.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 921         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005816157 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065685166 |\n",
      "|    clip_fraction        | 0.0661       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.678       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00813     |\n",
      "|    value_loss           | 38.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 929          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046205483 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 932          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039022746 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    value_loss           | 29.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 935          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038644979 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 938          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031790338 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0047      |\n",
      "|    value_loss           | 24.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 935         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004682272 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    value_loss           | 18.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 937          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023693466 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.41        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.61         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00242     |\n",
      "|    value_loss           | 24.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1445b0a44f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000) # Train model 40.000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88f0c",
   "metadata": {},
   "source": [
    "## 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5c1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models Acrobot', 'PPO_Model_Acrobot') # Locate path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path) #save model in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model #Delete model to simulate reloading in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e918fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env = env) # Loading again the model saved in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e372ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\PPO_Model_Acrobot'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e3ff9",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f08a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfzor\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-102.0, 40.52653451752321)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571bbd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d58ad",
   "metadata": {},
   "source": [
    "## 6. Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ec15aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-94.]\n",
      "Episode:2 Score:[-95.]\n",
      "Episode:3 Score:[-102.]\n",
      "Episode:4 Score:[-84.]\n",
      "Episode:5 Score:[-103.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a85edd",
   "metadata": {},
   "source": [
    "## 7. Viewing Logs in Tensorboard Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58c9c7-c264-4778-8ff4-4de8283e4d02",
   "metadata": {},
   "source": [
    "!tensorboard dev upload --logdir {Path_To_Log} --name \"Experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76132e1e",
   "metadata": {},
   "source": [
    "#### Execute in command line the tensorboard visualization http://localhost:6006 stop the cell to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930f33e",
   "metadata": {},
   "source": [
    "## 8. Adding a callback to the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aafbd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models Acrobot') #Where the best model is going to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c10c27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1) #Stop our training when we achieved a 200 rwd\n",
    "eval_callback = EvalCallback(env,  #callback that is triggered after each training run\n",
    "                            callback_on_new_best=stop_callback, #callback to run in the new best model\n",
    "                            eval_freq=10000, #Evaluation Frequency to 10.000 time steps\n",
    "                            best_model_save_path=save_path, # Save the model everytime there is a new best model\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47d655a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d390b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1821 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1229        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004805004 |\n",
      "|    clip_fraction        | 0.00708     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.18       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1134        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005298882 |\n",
      "|    clip_fraction        | 0.0132      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0257      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1088        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007157474 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | -500         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064165555 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.311        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00716     |\n",
      "|    value_loss           | 84.5         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 964   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 949          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022148206 |\n",
      "|    clip_fraction        | 0.00767      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    value_loss           | 83.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 952          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029923175 |\n",
      "|    clip_fraction        | 0.00625      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.412        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.9         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 70           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 953         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010149991 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    value_loss           | 61.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 955         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009801826 |\n",
      "|    clip_fraction        | 0.0698      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-176.20 +/- 162.84\n",
      "Episode length: 177.00 +/- 162.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 177          |\n",
      "|    mean_reward          | -176         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062225144 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.867       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00547     |\n",
      "|    value_loss           | 39.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 937   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 940          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063542137 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.838       |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 44.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 945          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060260226 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.812       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00601     |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 949         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005795566 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.787      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 33.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 948         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009812439 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.731      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    value_loss           | 35.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-142.80 +/- 100.13\n",
      "Episode length: 143.80 +/- 100.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 144         |\n",
      "|    mean_reward          | -143        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006545085 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.6         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    value_loss           | 23.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 941   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 944          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061968258 |\n",
      "|    clip_fraction        | 0.0726       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.61        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 28.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 947          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045891823 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.545       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 23.7         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 947       |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 38        |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0094289 |\n",
      "|    clip_fraction        | 0.101     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.497    |\n",
      "|    explained_variance   | 0.932     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.13      |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0106   |\n",
      "|    value_loss           | 19.4      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 950          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030694818 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-76.80 +/- 5.98\n",
      "Episode length: 77.80 +/- 5.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 77.8         |\n",
      "|    mean_reward          | -76.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047337064 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.422       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.67         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    value_loss           | 22.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 948   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14471ecb2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) # Training model with callback argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c853d34",
   "metadata": {},
   "source": [
    "## 9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b89ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])] #dictionary neural network for our custom actor=PI and valueFunctn\n",
    "                                                              #128 un/eachLayer (4Lyrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d94ba72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#associating this new_Arch to the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e6ac2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1383 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009211478 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.00776    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.87        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    value_loss           | 34          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061814575 |\n",
      "|    clip_fraction        | 0.0061       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -0.215       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.77         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 70.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 695          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023783576 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.73         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 63           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9040, episode_reward=-114.80 +/- 79.66\n",
      "Episode length: 115.80 +/- 79.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 116        |\n",
      "|    mean_reward          | -115       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9040       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00967844 |\n",
      "|    clip_fraction        | 0.0516     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.518      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.95       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00583   |\n",
      "|    value_loss           | 45         |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 662   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 656          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062269345 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.573        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 51.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01775448 |\n",
      "|    clip_fraction        | 0.0873     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.941     |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.3       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 36.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006066573 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.54        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 30.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006989442 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.787      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.38        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 26.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19040, episode_reward=-79.80 +/- 5.84\n",
      "Episode length: 80.80 +/- 5.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80.8        |\n",
      "|    mean_reward          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006345154 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.4         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 25.4        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 638   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008372182 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    value_loss           | 21.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 636          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055816593 |\n",
      "|    clip_fraction        | 0.0692       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00473     |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005902745 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.96        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053715697 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.491       |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00473     |\n",
      "|    value_loss           | 12.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=29040, episode_reward=-78.20 +/- 10.70\n",
      "Episode length: 79.20 +/- 10.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 79.2         |\n",
      "|    mean_reward          | -78.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037745605 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.8         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    value_loss           | 23.7         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 630   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002926888 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.55        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00654    |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 630          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045628445 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.77         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    value_loss           | 13.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027512033 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.87         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    value_loss           | 14.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006857996 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.54        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39040, episode_reward=-82.40 +/- 8.40\n",
      "Episode length: 83.40 +/- 8.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 83.4         |\n",
      "|    mean_reward          | -82.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033162832 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.309       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.73         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00314     |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 624   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14471ee9370>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37dd0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-90.]\n",
      "Episode:2 Score:[-89.]\n",
      "Episode:3 Score:[-97.]\n",
      "Episode:4 Score:[-82.]\n",
      "Episode:5 Score:[-85.]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with new architecture\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs) #using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "162d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f6006",
   "metadata": {},
   "source": [
    "## 10. Using DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99dc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN # DQN RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a7c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f211ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 5257     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 5310     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 5278     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 5206     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 3682     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 10080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 3887     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 12080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 3960     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 14080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 4092     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 16080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 3515     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 18080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 3628     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 20080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 3726     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 22080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 3796     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 24080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 3879     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 26080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 3576     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 28080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 3657     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 30080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 3741     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 32080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 3812     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 34062    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 3874     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 36062    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 3914     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 38062    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38080    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x14471ee98b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) #40.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01367999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DQN_Path = os.path.join('Training','Saved Models Acrobot', 'DQN_Model_Acrobot')\n",
    "model.save(DQN_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28dbf245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\DQN_Model_Acrobot'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e99b3835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-500.0, 0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d6c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6294bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x14471ee98b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f589c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-500.]\n",
      "Episode:2 Score:[-500.]\n",
      "Episode:3 Score:[-500.]\n",
      "Episode:4 Score:[-500.]\n",
      "Episode:5 Score:[-500.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b9b7def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a270150-b587-4caf-a356-8ba0089ea3b6",
   "metadata": {},
   "source": [
    "## 11. Using A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b46e926c-b634-427f-b689-92923940f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C # A2C RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d54f5866-782b-4d8c-850b-9d5e05b28f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43f937ec-bbef-40ea-a272-a46af72d4af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\A2C_3\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.0171   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -1.34e-08 |\n",
      "|    value_loss         | 2.25e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 601       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000151 |\n",
      "|    explained_variance | -0.0432   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -1.37e-08 |\n",
      "|    value_loss         | 1.46e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 590       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000142 |\n",
      "|    explained_variance | -0.0305   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -1.43e-08 |\n",
      "|    value_loss         | 1.84e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 585       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000129 |\n",
      "|    explained_variance | -0.0203   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -1.36e-08 |\n",
      "|    value_loss         | 2.35e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 585       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00014  |\n",
      "|    explained_variance | -0.0316   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 1.69e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 584       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000124 |\n",
      "|    explained_variance | -0.0079   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -1.09e-08 |\n",
      "|    value_loss         | 1.59e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 584       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000124 |\n",
      "|    explained_variance | -0.0194   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -1.12e-08 |\n",
      "|    value_loss         | 1.73e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 588       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | -0.013    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -1.11e-08 |\n",
      "|    value_loss         | 1.38e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 589       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000137 |\n",
      "|    explained_variance | -0.0179   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -1.16e-08 |\n",
      "|    value_loss         | 1.32e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 585       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | -0.0107   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -1.27e-08 |\n",
      "|    value_loss         | 2.03e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 582       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.0106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 2.12e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 584       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | -0.0197   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | -1.23e-08 |\n",
      "|    value_loss         | 1.69e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 584      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00014 |\n",
      "|    explained_variance | -0.0289  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -1.3e-08 |\n",
      "|    value_loss         | 1.58e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 582       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00013  |\n",
      "|    explained_variance | -0.0139   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -1.38e-08 |\n",
      "|    value_loss         | 2.17e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 581       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000136 |\n",
      "|    explained_variance | -0.0356   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -1.45e-08 |\n",
      "|    value_loss         | 2.13e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000129 |\n",
      "|    explained_variance | -0.0197   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | -1.21e-08 |\n",
      "|    value_loss         | 1.67e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 8080      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000134 |\n",
      "|    explained_variance | -0.0225   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17615     |\n",
      "|    policy_loss        | -1.29e-08 |\n",
      "|    value_loss         | 1.91e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000147 |\n",
      "|    explained_variance | -0.0382   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -1.47e-08 |\n",
      "|    value_loss         | 1.86e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 527       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | 7.45e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | -1.17e-08 |\n",
      "|    value_loss         | 1.74e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000138 |\n",
      "|    explained_variance | -0.0221   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -1.4e-08  |\n",
      "|    value_loss         | 1.98e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000138 |\n",
      "|    explained_variance | -0.0243   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -1.24e-08 |\n",
      "|    value_loss         | 1.62e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000147 |\n",
      "|    explained_variance | -0.0316   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -1.59e-08 |\n",
      "|    value_loss         | 2.21e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000129 |\n",
      "|    explained_variance | -0.0217   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | -1.27e-08 |\n",
      "|    value_loss         | 2.02e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000144 |\n",
      "|    explained_variance | -0.0316   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | -1.36e-08 |\n",
      "|    value_loss         | 1.7e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000138 |\n",
      "|    explained_variance | -0.0243   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -1.23e-08 |\n",
      "|    value_loss         | 1.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000135 |\n",
      "|    explained_variance | -0.0168   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -1.16e-08 |\n",
      "|    value_loss         | 1.5e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 541       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000145 |\n",
      "|    explained_variance | -0.0438   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -1.39e-08 |\n",
      "|    value_loss         | 1.7e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 543       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000133 |\n",
      "|    explained_variance | -0.00797  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -1.19e-08 |\n",
      "|    value_loss         | 1.55e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 543       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0209   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -1.13e-08 |\n",
      "|    value_loss         | 1.52e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 545       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | -0.0253   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -1.15e-08 |\n",
      "|    value_loss         | 1.51e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 546       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000138 |\n",
      "|    explained_variance | -0.0116   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -1.3e-08  |\n",
      "|    value_loss         | 1.73e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 547       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000144 |\n",
      "|    explained_variance | -0.0202   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -1.35e-08 |\n",
      "|    value_loss         | 1.62e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 547       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000133 |\n",
      "|    explained_variance | -0.021    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | -1.37e-08 |\n",
      "|    value_loss         | 2.16e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 548       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000143 |\n",
      "|    explained_variance | -0.0248   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -1.52e-08 |\n",
      "|    value_loss         | 2.1e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000155 |\n",
      "|    explained_variance | -0.0414   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | -1.45e-08 |\n",
      "|    value_loss         | 1.58e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00013  |\n",
      "|    explained_variance | -0.0202   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -1.17e-08 |\n",
      "|    value_loss         | 1.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 551       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000126 |\n",
      "|    explained_variance | 6.91e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | -1.2e-08  |\n",
      "|    value_loss         | 1.88e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=18080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 18080     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | -0.0112   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19615     |\n",
      "|    policy_loss        | -1.23e-08 |\n",
      "|    value_loss         | 1.87e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000126 |\n",
      "|    explained_variance | 0.000124  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -1.13e-08 |\n",
      "|    value_loss         | 1.68e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 527       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000137 |\n",
      "|    explained_variance | -0.033    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -1.24e-08 |\n",
      "|    value_loss         | 1.6e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 527       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000136 |\n",
      "|    explained_variance | -0.0183   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -1.35e-08 |\n",
      "|    value_loss         | 1.95e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 527       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.011    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -1.25e-08 |\n",
      "|    value_loss         | 1.92e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000136 |\n",
      "|    explained_variance | -0.0158   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20099     |\n",
      "|    policy_loss        | -1.27e-08 |\n",
      "|    value_loss         | 1.69e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00014  |\n",
      "|    explained_variance | -0.0251   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20199     |\n",
      "|    policy_loss        | -1.25e-08 |\n",
      "|    value_loss         | 1.5e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | -0.00714  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20299     |\n",
      "|    policy_loss        | -1.27e-08 |\n",
      "|    value_loss         | 2.01e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 532       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20399     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 2.09e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000141 |\n",
      "|    explained_variance | -0.0242   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20499     |\n",
      "|    policy_loss        | -1.29e-08 |\n",
      "|    value_loss         | 1.64e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000137 |\n",
      "|    explained_variance | -0.019    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20599     |\n",
      "|    policy_loss        | -1.34e-08 |\n",
      "|    value_loss         | 1.84e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000129 |\n",
      "|    explained_variance | -0.019    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20699     |\n",
      "|    policy_loss        | -1.2e-08  |\n",
      "|    value_loss         | 1.8e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000125 |\n",
      "|    explained_variance | 4.47e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20799     |\n",
      "|    policy_loss        | -1.43e-08 |\n",
      "|    value_loss         | 2.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | -0.0114   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20899     |\n",
      "|    policy_loss        | -1.25e-08 |\n",
      "|    value_loss         | 1.82e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | -0.00757  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20999     |\n",
      "|    policy_loss        | -1.2e-08  |\n",
      "|    value_loss         | 1.82e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000139 |\n",
      "|    explained_variance | -0.0324   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21099     |\n",
      "|    policy_loss        | -1.3e-08  |\n",
      "|    value_loss         | 1.64e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000153 |\n",
      "|    explained_variance | -0.039    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21199     |\n",
      "|    policy_loss        | -1.99e-08 |\n",
      "|    value_loss         | 3.01e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000124 |\n",
      "|    explained_variance | 6.91e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21299     |\n",
      "|    policy_loss        | -1.18e-08 |\n",
      "|    value_loss         | 1.87e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 541       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000139 |\n",
      "|    explained_variance | -0.0258   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21399     |\n",
      "|    policy_loss        | -1.2e-08  |\n",
      "|    value_loss         | 1.45e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 542       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000163 |\n",
      "|    explained_variance | -0.0407   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21499     |\n",
      "|    policy_loss        | -1.71e-08 |\n",
      "|    value_loss         | 2e-06     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 543       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000152 |\n",
      "|    explained_variance | -0.0339   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21599     |\n",
      "|    policy_loss        | -1.67e-08 |\n",
      "|    value_loss         | 2.36e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=28080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28080     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000155 |\n",
      "|    explained_variance | -0.036    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21615     |\n",
      "|    policy_loss        | -1.35e-08 |\n",
      "|    value_loss         | 1.33e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000144 |\n",
      "|    explained_variance | -0.0243   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21699     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 1.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000134 |\n",
      "|    explained_variance | -0.0219   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21799     |\n",
      "|    policy_loss        | -1.09e-08 |\n",
      "|    value_loss         | 1.38e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 529       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000129 |\n",
      "|    explained_variance | -0.0149   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21899     |\n",
      "|    policy_loss        | -1.26e-08 |\n",
      "|    value_loss         | 1.9e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | -0.0209   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21999     |\n",
      "|    policy_loss        | -1.37e-08 |\n",
      "|    value_loss         | 2.16e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | 2.59e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22099     |\n",
      "|    policy_loss        | -1.27e-08 |\n",
      "|    value_loss         | 2.03e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000149 |\n",
      "|    explained_variance | -0.039    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22199     |\n",
      "|    policy_loss        | -1.45e-08 |\n",
      "|    value_loss         | 1.76e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 532       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | 2.42e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22299     |\n",
      "|    policy_loss        | -1.31e-08 |\n",
      "|    value_loss         | 2.14e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000136 |\n",
      "|    explained_variance | -0.0161   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22399     |\n",
      "|    policy_loss        | -1.25e-08 |\n",
      "|    value_loss         | 1.61e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0177   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22499     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 2.08e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0194   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22599     |\n",
      "|    policy_loss        | -1.24e-08 |\n",
      "|    value_loss         | 1.74e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | 2.42e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22699     |\n",
      "|    policy_loss        | -1.32e-08 |\n",
      "|    value_loss         | 2.15e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000162 |\n",
      "|    explained_variance | -0.0608   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22799     |\n",
      "|    policy_loss        | -1.44e-08 |\n",
      "|    value_loss         | 1.52e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000146 |\n",
      "|    explained_variance | -0.0421   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22899     |\n",
      "|    policy_loss        | -1.31e-08 |\n",
      "|    value_loss         | 1.53e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0177   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22999     |\n",
      "|    policy_loss        | -1.33e-08 |\n",
      "|    value_loss         | 2.08e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000136 |\n",
      "|    explained_variance | -0.0261   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | -1.32e-08 |\n",
      "|    value_loss         | 1.89e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.0144   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23199     |\n",
      "|    policy_loss        | -1.28e-08 |\n",
      "|    value_loss         | 2e-06     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000127 |\n",
      "|    explained_variance | -0.0122   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23299     |\n",
      "|    policy_loss        | -1.09e-08 |\n",
      "|    value_loss         | 1.54e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.00702  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23399     |\n",
      "|    policy_loss        | -1.3e-08  |\n",
      "|    value_loss         | 2.09e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000143 |\n",
      "|    explained_variance | -0.0272   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23499     |\n",
      "|    policy_loss        | -1.39e-08 |\n",
      "|    value_loss         | 1.78e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000131 |\n",
      "|    explained_variance | -0.0206   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23599     |\n",
      "|    policy_loss        | -1.16e-08 |\n",
      "|    value_loss         | 1.57e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 38080     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000128 |\n",
      "|    explained_variance | -0.0217   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23615     |\n",
      "|    policy_loss        | -1.28e-08 |\n",
      "|    value_loss         | 2.02e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000148 |\n",
      "|    explained_variance | -0.0243   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23699     |\n",
      "|    policy_loss        | -1.39e-08 |\n",
      "|    value_loss         | 1.65e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000134 |\n",
      "|    explained_variance | -0.018    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23799     |\n",
      "|    policy_loss        | -1.36e-08 |\n",
      "|    value_loss         | 2.03e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000152 |\n",
      "|    explained_variance | -0.046    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23899     |\n",
      "|    policy_loss        | -1.5e-08  |\n",
      "|    value_loss         | 1.86e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000134 |\n",
      "|    explained_variance | -0.0158   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23999     |\n",
      "|    policy_loss        | -1.23e-08 |\n",
      "|    value_loss         | 1.68e-06  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x14474542910>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) #40.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bec4b96-81ff-45ec-95c8-8c9329f46ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_Path = os.path.join('Training','Saved Models Acrobot', 'A2C_Model_Acrobot')\n",
    "model.save(A2C_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0ba0d83-bd05-431a-b036-22bffc5966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\A2C_Model_Acrobot'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2C_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38fa67cd-e9bf-4b59-a95e-33fb40cdd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-500.0, 0.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b66dc9f-d6c0-42a9-af7d-e44ccb4e9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49f38710-030c-47c0-8076-8a91252bf2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x14474542910>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b9bc2cf-8a3d-4d12-bd1b-74b4df3da29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-500.]\n",
      "Episode:2 Score:[-500.]\n",
      "Episode:3 Score:[-500.]\n",
      "Episode:4 Score:[-500.]\n",
      "Episode:5 Score:[-500.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9fdfcc1-8037-4ba5-b09b-d945d2c0b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
