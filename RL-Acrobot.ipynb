{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6575b107",
   "metadata": {},
   "source": [
    "# TFM | Reinforcement Learning | Daniel Zorrilla | Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0745f64",
   "metadata": {},
   "source": [
    "## Installing additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aee1d",
   "metadata": {},
   "source": [
    "###### Installing stable baselines and pyglet library for developing games and other visually-rich applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8310d25a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.22.4)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (9.0.1)\n",
      "Requirement already satisfied: ale-py~=0.7.4 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.7.5)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.9.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.5.5.64)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (4.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.4->stable-baselines3[extra]) (3.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.20.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.42.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (61.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.2.0->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.5.18.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from torch>=1.8.1->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2021.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f561e7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in c:\\users\\dfzor\\anaconda3\\lib\\site-packages (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58854c-77f5-4004-b9e8-a905ef61a7bc",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c101b-20bf-4dfe-9993-23ae40b186c3",
   "metadata": {},
   "source": [
    "## 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6267c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # provides a way of using OS dependent functionality. (files)\n",
    "import gym # Open AI gym\n",
    "from stable_baselines3 import PPO #PPO RL Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Creates a simple vectorized wrapper for multiple environments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # Test how well a model is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3670b76",
   "metadata": {},
   "source": [
    "## 2. Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1625fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Acrobot-v1' # Naming the Acrobot environment\n",
    "env = gym.make(environment_name) # Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-500.0\n",
      "Episode:2 Score:-481.0\n",
      "Episode:3 Score:-500.0\n",
      "Episode:4 Score:-500.0\n",
      "Episode:5 Score:-500.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5  # Number of episodes\n",
    "for episode in range (1, episodes+1): # Resetting environment  \n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # While episode active\n",
    "        env.render() # Visualizing environment\n",
    "        action = env.action_space.sample() # Creating sample actions\n",
    "        n_state, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward # Getting score\n",
    "    print('Episode:{} Score:{}'.format(episode,score)) # Printing episode and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c331912",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # Closing the render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c237420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # Understanding the action space of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d11c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Action random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92fef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # Understanding the observation space of this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db8c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.73386836,  -0.4865981 ,   0.3885829 ,   0.76086515,\n",
       "        -9.824807  , -13.917628  ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7efa8e",
   "metadata": {},
   "source": [
    "## 3. Train and create RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b6c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Acrobot') #where it is saved the tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e57a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Acrobot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e153fa",
   "metadata": {},
   "source": [
    "#### Install Pytorch *conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffe8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name) # Create environment\n",
    "env = DummyVecEnv([lambda: env]) # Wrapped environment using DummyVecEnv\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path) # Creating PPO Algorithm with MultiLayerPerceptron Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6611270",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 695  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008623523 |\n",
      "|    clip_fraction        | 0.0198      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0207     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007908847 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 431         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006256602 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 92.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008825958 |\n",
      "|    clip_fraction        | 0.0891      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    value_loss           | 87.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 415          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036850316 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0646       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.3         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    value_loss           | 81.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 410         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007822596 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 407         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009961834 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 404          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059867436 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.927       |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.3         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00855     |\n",
      "|    value_loss           | 67.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010237453 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 49.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00612314 |\n",
      "|    clip_fraction        | 0.0376     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.849     |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.4       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00424   |\n",
      "|    value_loss           | 40.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008865882 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.825      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00821    |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006953523 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00451    |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011246221 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.768      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 396          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077698296 |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.674       |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.1         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00714     |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 395          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059819994 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00502     |\n",
      "|    value_loss           | 34.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007401972 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.75        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 27.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005191669 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 29.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 394          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060601975 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.02         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    value_loss           | 28.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004428178 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.68        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 392          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029178257 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    value_loss           | 33.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 392          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022470113 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.294       |\n",
      "|    explained_variance   | 0.703        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 36.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 391          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028518664 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.9         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    value_loss           | 41.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 390          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032101918 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.1         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    value_loss           | 34.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 390          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 131          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006686944 |\n",
      "|    clip_fraction        | 0.00483      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.38         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000959    |\n",
      "|    value_loss           | 33           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fa03cea310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000) # Train model 100.000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88f0c",
   "metadata": {},
   "source": [
    "## 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5c1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models Acrobot', 'PPO_Model_Acrobot') # Locate path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path) #save model in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model #Delete model to simulate reloading in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e918fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env = env) # Loading again the model saved in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e372ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\PPO_Model_Acrobot'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e3ff9",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f08a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfzor\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-115.4, 48.565831610299846)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571bbd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d58ad",
   "metadata": {},
   "source": [
    "## 6. Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ec15aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-150.]\n",
      "Episode:2 Score:[-131.]\n",
      "Episode:3 Score:[-117.]\n",
      "Episode:4 Score:[-176.]\n",
      "Episode:5 Score:[-78.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a85edd",
   "metadata": {},
   "source": [
    "## 7. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06431c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1') # Locating PPO_1 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b0e0498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Acrobot\\\\PPO_1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76132e1e",
   "metadata": {},
   "source": [
    "#### Execute in command line the tensorboard visualization http://localhost:6006 stop the cell to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930f33e",
   "metadata": {},
   "source": [
    "## 8. Adding a callback to the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafbd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models Acrobot') #Where the best model is going to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10c27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1) #Stop our training when we achieved a 200 rwd\n",
    "eval_callback = EvalCallback(env,  #callback that is triggered after each training run\n",
    "                            callback_on_new_best=stop_callback, #callback to run in the new best model\n",
    "                            eval_freq=10000, #Evaluation Frequency to 10.000 time steps\n",
    "                            best_model_save_path=save_path, # Save the model everytime there is a new best model\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d655a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6d390b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 735  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 504          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072079394 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.0122      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043703094 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00108     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045889867 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 99.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -500        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006617453 |\n",
      "|    clip_fraction        | 0.0211      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.0167     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    value_loss           | 83.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 380   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 380          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063546267 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000842     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    value_loss           | 71.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006985696 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    value_loss           | 73.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008045086 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0357      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00952    |\n",
      "|    value_loss           | 91          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006159018 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    value_loss           | 73.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | -500        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007041198 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 68.7        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015317634 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.865      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    value_loss           | 51.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 366          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079051135 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.795       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00471     |\n",
      "|    value_loss           | 36.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006486193 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.735      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 41.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006482162 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-180.00 +/- 75.01\n",
      "Episode length: 181.00 +/- 75.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 181         |\n",
      "|    mean_reward          | -180        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005600892 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 89         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00471781 |\n",
      "|    clip_fraction        | 0.0342     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.59      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.3       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0048    |\n",
      "|    value_loss           | 25.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004685725 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.5         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 22.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006415343 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 367          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037300976 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.92         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    value_loss           | 24.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-81.20 +/- 5.71\n",
      "Episode length: 82.20 +/- 5.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 82.2        |\n",
      "|    mean_reward          | -81.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004549049 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.12        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 367          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034501087 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.94         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 16.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 368          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032573685 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.73         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    value_loss           | 19.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 368          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021460797 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.342       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.64         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 20.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 369          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 133          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037954485 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.336       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    value_loss           | 24.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-84.60 +/- 11.16\n",
      "Episode length: 85.60 +/- 11.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 85.6        |\n",
      "|    mean_reward          | -84.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003717291 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.94        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 368   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fa114b2c70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) # Training model with callback argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c853d34",
   "metadata": {},
   "source": [
    "## 9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b89ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])] #dictionary neural network for our custom actor=PI and valueFunctn\n",
    "                                                              #128 un/eachLayer (4Lyrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d94ba72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#associating this new_Arch to the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77e6ac2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 523  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012025047 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0575      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.69        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    value_loss           | 40.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006703121 |\n",
      "|    clip_fraction        | 0.0084      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0581      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.4         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000716   |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024607806 |\n",
      "|    clip_fraction        | 0.000342     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.48         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000592    |\n",
      "|    value_loss           | 51.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8800, episode_reward=-116.40 +/- 7.89\n",
      "Episode length: 117.40 +/- 7.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 117         |\n",
      "|    mean_reward          | -116        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8800        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009639819 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.68        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00418    |\n",
      "|    value_loss           | 28.1        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 263   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005710507 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062821084 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.58         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 40.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007208059 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.61        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    value_loss           | 38.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007766215 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18800, episode_reward=-83.40 +/- 3.93\n",
      "Episode length: 84.40 +/- 3.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 84.4        |\n",
      "|    mean_reward          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015605943 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.856      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 36.1        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 250   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 249        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00998735 |\n",
      "|    clip_fraction        | 0.0573     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.77      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.94       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    value_loss           | 19.4       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102971075 |\n",
      "|    clip_fraction        | 0.0761       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.683       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00844     |\n",
      "|    value_loss           | 31.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065770303 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.9          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00512     |\n",
      "|    value_loss           | 22.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006824577 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.22        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28800, episode_reward=-80.20 +/- 7.08\n",
      "Episode length: 81.20 +/- 7.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 81.2        |\n",
      "|    mean_reward          | -80.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004238764 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.32        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 246   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 124   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004925798 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039585964 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.16         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    value_loss           | 14.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008819457 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38800, episode_reward=-84.00 +/- 6.69\n",
      "Episode length: 85.00 +/- 6.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 85           |\n",
      "|    mean_reward          | -84          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38800        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054389485 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.37         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 18.5         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 243   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 159   |\n",
      "|    total_timesteps | 38912 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076345652 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.05         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00625     |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007462284 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.89        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 15.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024978765 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.33         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015204438 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.94         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 17.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48800, episode_reward=-85.00 +/- 8.34\n",
      "Episode length: 86.00 +/- 8.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 86           |\n",
      "|    mean_reward          | -85          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48800        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042490475 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.25         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 12           |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 242   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 202   |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033666566 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.55         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    value_loss           | 13.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fa11537b80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37dd0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-87.]\n",
      "Episode:2 Score:[-90.]\n",
      "Episode:3 Score:[-78.]\n",
      "Episode:4 Score:[-85.]\n",
      "Episode:5 Score:[-96.]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with new architecture\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs) #using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "162d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f6006",
   "metadata": {},
   "source": [
    "## 10. Using DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99dc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN # DQN RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a7c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f211ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2250     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 2274     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2270     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1443     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 8100     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1559     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 10100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1645     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 12100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1698     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 14100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1754     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 16100    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1501     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 18100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1552     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 20100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1599     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 22100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1635     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 24100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1659     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 26100    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1508     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 28100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1543     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 30100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1573     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 32100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1602     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 34100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1629     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 36100    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1518     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 38100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1543     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 40100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 1567     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 42100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1590     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 44100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1611     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 46100    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -500     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1522     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 48100    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1fa1153f760>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) #50.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01367999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DQN_Path = os.path.join('Training','Saved Models Acrobot', 'DQN_Model_Acrobot')\n",
    "model.save(DQN_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28dbf245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\DQN_Model_Acrobot'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e99b3835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-500.0, 0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d6c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6294bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1fa1153f760>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f589c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-500.]\n",
      "Episode:2 Score:[-500.]\n",
      "Episode:3 Score:[-500.]\n",
      "Episode:4 Score:[-500.]\n",
      "Episode:5 Score:[-500.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b9b7def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a270150-b587-4caf-a356-8ba0089ea3b6",
   "metadata": {},
   "source": [
    "## 11. Using A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b46e926c-b634-427f-b689-92923940f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C # A2C RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d54f5866-782b-4d8c-850b-9d5e05b28f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43f937ec-bbef-40ea-a272-a46af72d4af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Acrobot\\A2C_3\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0074   |\n",
      "|    explained_variance | -0.0329   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20099     |\n",
      "|    policy_loss        | -7.09e-07 |\n",
      "|    value_loss         | 6.78e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0073   |\n",
      "|    explained_variance | -0.0936   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20199     |\n",
      "|    policy_loss        | -5.86e-07 |\n",
      "|    value_loss         | 4.77e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00817  |\n",
      "|    explained_variance | 0.103     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20299     |\n",
      "|    policy_loss        | -3.87e-07 |\n",
      "|    value_loss         | 2.14e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 368      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00723 |\n",
      "|    explained_variance | -0.052   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | -5e-07   |\n",
      "|    value_loss         | 3.81e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 367       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00728  |\n",
      "|    explained_variance | -0.0109   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20499     |\n",
      "|    policy_loss        | -7.72e-07 |\n",
      "|    value_loss         | 9.02e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00867  |\n",
      "|    explained_variance | -0.0526   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20599     |\n",
      "|    policy_loss        | -9.55e-07 |\n",
      "|    value_loss         | 9.74e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 369       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00607  |\n",
      "|    explained_variance | -0.0573   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20699     |\n",
      "|    policy_loss        | -7.02e-07 |\n",
      "|    value_loss         | 1.05e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 369       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00666  |\n",
      "|    explained_variance | 0.0609    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20799     |\n",
      "|    policy_loss        | -4.62e-07 |\n",
      "|    value_loss         | 4.34e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 370       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00649  |\n",
      "|    explained_variance | -0.0311   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20899     |\n",
      "|    policy_loss        | -4.83e-07 |\n",
      "|    value_loss         | 4.58e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00615  |\n",
      "|    explained_variance | -0.109    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20999     |\n",
      "|    policy_loss        | -3.75e-07 |\n",
      "|    value_loss         | 3.06e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.007    |\n",
      "|    explained_variance | 0.0773    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21099     |\n",
      "|    policy_loss        | -4.46e-07 |\n",
      "|    value_loss         | 3.76e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00843  |\n",
      "|    explained_variance | 0.057     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21199     |\n",
      "|    policy_loss        | -9.11e-07 |\n",
      "|    value_loss         | 9.05e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00852  |\n",
      "|    explained_variance | 0.135     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21299     |\n",
      "|    policy_loss        | -6.17e-07 |\n",
      "|    value_loss         | 4.44e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00722  |\n",
      "|    explained_variance | -0.128    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21399     |\n",
      "|    policy_loss        | -5.91e-07 |\n",
      "|    value_loss         | 5.36e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0081   |\n",
      "|    explained_variance | 0.0106    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21499     |\n",
      "|    policy_loss        | -8.11e-07 |\n",
      "|    value_loss         | 9.04e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=7600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 7600      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00845  |\n",
      "|    explained_variance | 0.0398    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21519     |\n",
      "|    policy_loss        | -5.85e-07 |\n",
      "|    value_loss         | 4.04e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 329       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0055   |\n",
      "|    explained_variance | 0.0529    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21599     |\n",
      "|    policy_loss        | -4.23e-07 |\n",
      "|    value_loss         | 5.88e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 331       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00562  |\n",
      "|    explained_variance | -0.0332   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21699     |\n",
      "|    policy_loss        | -4.78e-07 |\n",
      "|    value_loss         | 6.21e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00389  |\n",
      "|    explained_variance | -0.123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21799     |\n",
      "|    policy_loss        | -2.73e-07 |\n",
      "|    value_loss         | 4.7e-07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00281  |\n",
      "|    explained_variance | -0.0181   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21899     |\n",
      "|    policy_loss        | -2.24e-07 |\n",
      "|    value_loss         | 7.25e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00326  |\n",
      "|    explained_variance | 0.0688    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21999     |\n",
      "|    policy_loss        | -3.04e-07 |\n",
      "|    value_loss         | 9.78e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00285  |\n",
      "|    explained_variance | -0.0412   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22099     |\n",
      "|    policy_loss        | -2.16e-07 |\n",
      "|    value_loss         | 5.85e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00208  |\n",
      "|    explained_variance | 0.0136    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22199     |\n",
      "|    policy_loss        | -1.51e-07 |\n",
      "|    value_loss         | 5.72e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00252  |\n",
      "|    explained_variance | 0.083     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22299     |\n",
      "|    policy_loss        | -1.49e-07 |\n",
      "|    value_loss         | 4.69e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00198  |\n",
      "|    explained_variance | -0.0293   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22399     |\n",
      "|    policy_loss        | -1.28e-07 |\n",
      "|    value_loss         | 4.85e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 344      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00238 |\n",
      "|    explained_variance | 0.113    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | -1.6e-07 |\n",
      "|    value_loss         | 5.06e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 345       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00301  |\n",
      "|    explained_variance | -0.142    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22599     |\n",
      "|    policy_loss        | -2.51e-07 |\n",
      "|    value_loss         | 6.31e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 346       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00273  |\n",
      "|    explained_variance | -0.0504   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22699     |\n",
      "|    policy_loss        | -1.76e-07 |\n",
      "|    value_loss         | 4.23e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 347       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00429  |\n",
      "|    explained_variance | -0.139    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22799     |\n",
      "|    policy_loss        | -3.93e-07 |\n",
      "|    value_loss         | 7.23e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 348      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00469 |\n",
      "|    explained_variance | 0.0272   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -1.7e-07 |\n",
      "|    value_loss         | 1.23e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 349       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0045   |\n",
      "|    explained_variance | -0.13     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22999     |\n",
      "|    policy_loss        | -2.34e-07 |\n",
      "|    value_loss         | 2.29e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 349       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00519  |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | -2.42e-07 |\n",
      "|    value_loss         | 2.27e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 350      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00433 |\n",
      "|    explained_variance | -0.162   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | -2.5e-07 |\n",
      "|    value_loss         | 3.06e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 350       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00424  |\n",
      "|    explained_variance | -0.111    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23299     |\n",
      "|    policy_loss        | -2.92e-07 |\n",
      "|    value_loss         | 4.29e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00463 |\n",
      "|    explained_variance | -0.0198  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | -5e-07   |\n",
      "|    value_loss         | 1.11e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 351       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00434  |\n",
      "|    explained_variance | -0.0491   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23499     |\n",
      "|    policy_loss        | -3.96e-07 |\n",
      "|    value_loss         | 7.52e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=17600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 17600     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00443  |\n",
      "|    explained_variance | -0.0325   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23519     |\n",
      "|    policy_loss        | -3.11e-07 |\n",
      "|    value_loss         | 4.34e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00446  |\n",
      "|    explained_variance | -0.0415   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23599     |\n",
      "|    policy_loss        | -3.42e-07 |\n",
      "|    value_loss         | 5.6e-07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00456  |\n",
      "|    explained_variance | 0.0243    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23699     |\n",
      "|    policy_loss        | -2.89e-07 |\n",
      "|    value_loss         | 3.74e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00302 |\n",
      "|    explained_variance | 0.0163   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | -1.8e-07 |\n",
      "|    value_loss         | 3.67e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00219  |\n",
      "|    explained_variance | -0.106    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23899     |\n",
      "|    policy_loss        | -9.39e-08 |\n",
      "|    value_loss         | 2.1e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00248 |\n",
      "|    explained_variance | 0.0163   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | -1.5e-07 |\n",
      "|    value_loss         | 4.26e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00243  |\n",
      "|    explained_variance | -0.0253   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24099     |\n",
      "|    policy_loss        | -1.94e-07 |\n",
      "|    value_loss         | 6.97e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0027   |\n",
      "|    explained_variance | 0.046     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24199     |\n",
      "|    policy_loss        | -1.57e-07 |\n",
      "|    value_loss         | 4.17e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00241  |\n",
      "|    explained_variance | 0.0158    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24299     |\n",
      "|    policy_loss        | -1.46e-07 |\n",
      "|    value_loss         | 4e-07     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00256  |\n",
      "|    explained_variance | 0.067     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24399     |\n",
      "|    policy_loss        | -1.33e-07 |\n",
      "|    value_loss         | 3.37e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00269  |\n",
      "|    explained_variance | 0.0462    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24499     |\n",
      "|    policy_loss        | -2.07e-07 |\n",
      "|    value_loss         | 7.24e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00228  |\n",
      "|    explained_variance | -0.102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24599     |\n",
      "|    policy_loss        | -1.43e-07 |\n",
      "|    value_loss         | 4.16e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 342       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00333  |\n",
      "|    explained_variance | -0.252    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24699     |\n",
      "|    policy_loss        | -1.33e-07 |\n",
      "|    value_loss         | 1.52e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00388  |\n",
      "|    explained_variance | 0.0921    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24799     |\n",
      "|    policy_loss        | -2.22e-07 |\n",
      "|    value_loss         | 3.58e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00355  |\n",
      "|    explained_variance | 0.0221    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24899     |\n",
      "|    policy_loss        | -4.05e-07 |\n",
      "|    value_loss         | 1.3e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00333  |\n",
      "|    explained_variance | -0.0267   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24999     |\n",
      "|    policy_loss        | -2.63e-07 |\n",
      "|    value_loss         | 6.07e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00406  |\n",
      "|    explained_variance | 0.157     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25099     |\n",
      "|    policy_loss        | -2.33e-07 |\n",
      "|    value_loss         | 3.45e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00326  |\n",
      "|    explained_variance | -0.0717   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25199     |\n",
      "|    policy_loss        | -1.91e-07 |\n",
      "|    value_loss         | 3.57e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0033  |\n",
      "|    explained_variance | -0.183   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | -1.9e-07 |\n",
      "|    value_loss         | 3.2e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 345       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00394  |\n",
      "|    explained_variance | 0.000632  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25399     |\n",
      "|    policy_loss        | -2.34e-07 |\n",
      "|    value_loss         | 3.23e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 345       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00403  |\n",
      "|    explained_variance | 0.0579    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25499     |\n",
      "|    policy_loss        | -3.46e-07 |\n",
      "|    value_loss         | 7.28e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=27600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 27600     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00401  |\n",
      "|    explained_variance | 0.0875    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25519     |\n",
      "|    policy_loss        | -2.24e-07 |\n",
      "|    value_loss         | 3.05e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0045   |\n",
      "|    explained_variance | 0.0716    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25599     |\n",
      "|    policy_loss        | -4.77e-07 |\n",
      "|    value_loss         | 1.13e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00388  |\n",
      "|    explained_variance | 0.0248    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25699     |\n",
      "|    policy_loss        | -2.42e-07 |\n",
      "|    value_loss         | 3.89e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0038   |\n",
      "|    explained_variance | -0.0413   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25799     |\n",
      "|    policy_loss        | -1.38e-07 |\n",
      "|    value_loss         | 1.43e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00454 |\n",
      "|    explained_variance | 0.149    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | -2.6e-07 |\n",
      "|    value_loss         | 3.4e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0038   |\n",
      "|    explained_variance | -0.0327   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25999     |\n",
      "|    policy_loss        | -3.02e-07 |\n",
      "|    value_loss         | 6.1e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.004   |\n",
      "|    explained_variance | 0.00532  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | -3.7e-07 |\n",
      "|    value_loss         | 9.33e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 338      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00432 |\n",
      "|    explained_variance | 0.0802   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26199    |\n",
      "|    policy_loss        | -3.6e-07 |\n",
      "|    value_loss         | 6.99e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00394  |\n",
      "|    explained_variance | 0.0157    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26299     |\n",
      "|    policy_loss        | -2.56e-07 |\n",
      "|    value_loss         | 4.26e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00383  |\n",
      "|    explained_variance | 0.000692  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26399     |\n",
      "|    policy_loss        | -2.08e-07 |\n",
      "|    value_loss         | 2.98e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00446  |\n",
      "|    explained_variance | 0.171     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26499     |\n",
      "|    policy_loss        | -1.59e-07 |\n",
      "|    value_loss         | 1.45e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00412  |\n",
      "|    explained_variance | 0.0304    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26599     |\n",
      "|    policy_loss        | -2.81e-07 |\n",
      "|    value_loss         | 4.49e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00361  |\n",
      "|    explained_variance | -0.167    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26699     |\n",
      "|    policy_loss        | -1.74e-07 |\n",
      "|    value_loss         | 2.33e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00372  |\n",
      "|    explained_variance | -0.0366   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26799     |\n",
      "|    policy_loss        | -2.65e-07 |\n",
      "|    value_loss         | 4.94e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00488  |\n",
      "|    explained_variance | -0.0924   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26899     |\n",
      "|    policy_loss        | -4.38e-07 |\n",
      "|    value_loss         | 7.63e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00691 |\n",
      "|    explained_variance | 0.00733  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | -5.1e-07 |\n",
      "|    value_loss         | 4.76e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 342       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00821  |\n",
      "|    explained_variance | -0.451    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27099     |\n",
      "|    policy_loss        | -3.38e-07 |\n",
      "|    value_loss         | 1.37e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 342       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0089   |\n",
      "|    explained_variance | -0.179    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27199     |\n",
      "|    policy_loss        | -4.93e-07 |\n",
      "|    value_loss         | 2.37e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00871  |\n",
      "|    explained_variance | -0.257    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27299     |\n",
      "|    policy_loss        | -4.62e-07 |\n",
      "|    value_loss         | 2.19e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0087   |\n",
      "|    explained_variance | -0.191    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27399     |\n",
      "|    policy_loss        | -5.26e-07 |\n",
      "|    value_loss         | 2.89e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00899  |\n",
      "|    explained_variance | -0.0974   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27499     |\n",
      "|    policy_loss        | -7.75e-07 |\n",
      "|    value_loss         | 5.37e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=37600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 37600     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00928  |\n",
      "|    explained_variance | -0.0957   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27519     |\n",
      "|    policy_loss        | -6.24e-07 |\n",
      "|    value_loss         | 3.31e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0102   |\n",
      "|    explained_variance | 0.0974    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27599     |\n",
      "|    policy_loss        | -1.06e-06 |\n",
      "|    value_loss         | 7.94e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0105   |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27699     |\n",
      "|    policy_loss        | -7.62e-07 |\n",
      "|    value_loss         | 4.15e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0091   |\n",
      "|    explained_variance | -0.142    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27799     |\n",
      "|    policy_loss        | -3.99e-07 |\n",
      "|    value_loss         | 1.53e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00969  |\n",
      "|    explained_variance | 0.0404    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27899     |\n",
      "|    policy_loss        | -6.87e-07 |\n",
      "|    value_loss         | 3.78e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00975  |\n",
      "|    explained_variance | 0.047     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27999     |\n",
      "|    policy_loss        | -7.18e-07 |\n",
      "|    value_loss         | 4.03e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0126   |\n",
      "|    explained_variance | -0.0159   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28099     |\n",
      "|    policy_loss        | -9.07e-07 |\n",
      "|    value_loss         | 3.97e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0125   |\n",
      "|    explained_variance | -0.0211   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28199     |\n",
      "|    policy_loss        | -1.48e-06 |\n",
      "|    value_loss         | 9.63e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0206   |\n",
      "|    explained_variance | -0.0825   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28299     |\n",
      "|    policy_loss        | -2.06e-06 |\n",
      "|    value_loss         | 6.54e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0215   |\n",
      "|    explained_variance | 0.056     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28399     |\n",
      "|    policy_loss        | -3.57e-06 |\n",
      "|    value_loss         | 1.71e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0197  |\n",
      "|    explained_variance | -0.0371  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | 1.72e-06 |\n",
      "|    value_loss         | 4.78e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0146  |\n",
      "|    explained_variance | -0.208   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | 1.12e-06 |\n",
      "|    value_loss         | 4.36e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0112  |\n",
      "|    explained_variance | 0.04     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | 8.28e-07 |\n",
      "|    value_loss         | 3.55e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 340      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0105  |\n",
      "|    explained_variance | 0.111    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 7.4e-07  |\n",
      "|    value_loss         | 3.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 340      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0242  |\n",
      "|    explained_variance | 0.0675   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28899    |\n",
      "|    policy_loss        | 1.42e-06 |\n",
      "|    value_loss         | 1.78e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 340      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 132      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0182  |\n",
      "|    explained_variance | 0.0617   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | 2.65e-06 |\n",
      "|    value_loss         | 1.27e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0167  |\n",
      "|    explained_variance | 1.82e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | 1.8e-06  |\n",
      "|    value_loss         | 7.3e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.017   |\n",
      "|    explained_variance | 0.000466 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | 1.41e-06 |\n",
      "|    value_loss         | 4.57e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 136      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0219  |\n",
      "|    explained_variance | 0.0883   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | 2.41e-06 |\n",
      "|    value_loss         | 6.57e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 342      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0204  |\n",
      "|    explained_variance | 2.19e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | 2.12e-06 |\n",
      "|    value_loss         | 5.93e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 342      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0163  |\n",
      "|    explained_variance | 0.195    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | 8.63e-07 |\n",
      "|    value_loss         | 1.62e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=47600, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0173  |\n",
      "|    explained_variance | 0.133    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29519    |\n",
      "|    policy_loss        | 9.37e-07 |\n",
      "|    value_loss         | 1.7e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.015   |\n",
      "|    explained_variance | 0.0336   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | 1.11e-06 |\n",
      "|    value_loss         | 3.41e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0134  |\n",
      "|    explained_variance | 0.176    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | 7.58e-07 |\n",
      "|    value_loss         | 1.98e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0141  |\n",
      "|    explained_variance | -0.0172  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | 1.02e-06 |\n",
      "|    value_loss         | 3.7e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 146      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0197  |\n",
      "|    explained_variance | 0.0837   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | 1.04e-06 |\n",
      "|    value_loss         | 1.63e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0212  |\n",
      "|    explained_variance | -0.0451  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | 2.21e-06 |\n",
      "|    value_loss         | 6.48e-07 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1fa1155f160>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) #First 20.000 then 200.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bec4b96-81ff-45ec-95c8-8c9329f46ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_Path = os.path.join('Training','Saved Models Acrobot', 'A2C_Model_Acrobot')\n",
    "model.save(A2C_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0ba0d83-bd05-431a-b036-22bffc5966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models Acrobot\\\\A2C_Model_Acrobot'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2C_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38fa67cd-e9bf-4b59-a95e-33fb40cdd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-500.0, 0.0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b66dc9f-d6c0-42a9-af7d-e44ccb4e9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49f38710-030c-47c0-8076-8a91252bf2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1fa1155f160>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b9bc2cf-8a3d-4d12-bd1b-74b4df3da29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-500.]\n",
      "Episode:2 Score:[-500.]\n",
      "Episode:3 Score:[-500.]\n",
      "Episode:4 Score:[-500.]\n",
      "Episode:5 Score:[-500.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9fdfcc1-8037-4ba5-b09b-d945d2c0b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552eb145-6103-449a-983e-fa83f2dbe640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
