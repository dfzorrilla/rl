{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6575b107",
   "metadata": {},
   "source": [
    "# TFM | Reinforcement Learning | Daniel Zorrilla | MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0745f64",
   "metadata": {},
   "source": [
    "## Installing additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aee1d",
   "metadata": {},
   "source": [
    "###### Installing stable baselines and pyglet library for developing games and other visually-rich applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561e7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58854c-77f5-4004-b9e8-a905ef61a7bc",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c101b-20bf-4dfe-9993-23ae40b186c3",
   "metadata": {},
   "source": [
    "## 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6267c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # provides a way of using OS dependent functionality. (files)\n",
    "import gym # Open AI gym\n",
    "from stable_baselines3 import PPO #PPO RL Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Creates a simple vectorized wrapper for multiple environments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # Test how well a model is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3670b76",
   "metadata": {},
   "source": [
    "## 2. Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1625fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'MountainCar-v0' # Naming the MountainCar environment\n",
    "env = gym.make(environment_name) # Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-200.0\n",
      "Episode:2 Score:-200.0\n",
      "Episode:3 Score:-200.0\n",
      "Episode:4 Score:-200.0\n",
      "Episode:5 Score:-200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5  # Number of episodes\n",
    "for episode in range (1, episodes+1): # Resetting environment  \n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # While episode active\n",
    "        env.render() # Visualizing environment\n",
    "        action = env.action_space.sample() # Creating sample actions\n",
    "        n_state, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward # Getting score\n",
    "    print('Episode:{} Score:{}'.format(episode,score)) # Printing episode and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c331912",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # Closing the render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c237420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # Understanding the action space of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d11c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Action random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92fef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # Understanding the observation space of this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db8c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45368654,  0.04150966], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7efa8e",
   "metadata": {},
   "source": [
    "## 3. Train and create RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b6c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'MountainCar') #where it is saved the tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e57a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\MountainCar'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e153fa",
   "metadata": {},
   "source": [
    "#### Install Pytorch *conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffe8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name) # Create environment\n",
    "env = DummyVecEnv([lambda: env]) # Wrapped environment using DummyVecEnv\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path) # Creating PPO Algorithm with MultiLayerPerceptron Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6611270",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 870  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 567         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008367155 |\n",
      "|    clip_fraction        | 0.00122     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.000112    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 504         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010893539 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0037      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011984905 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.023        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 6.19e-05     |\n",
      "|    value_loss           | 83.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011407478 |\n",
      "|    clip_fraction        | 0.0199      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.000509    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 68.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036073956 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00328     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.41         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000386    |\n",
      "|    value_loss           | 54           |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 455           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022021792 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | 0.000143      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.95          |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 0.000109      |\n",
      "|    value_loss           | 41.2          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 450         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010503714 |\n",
      "|    clip_fraction        | 0.00918     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.00305    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988062 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.000873   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 444        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00719398 |\n",
      "|    clip_fraction        | 0.00928    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.00792    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.52       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    value_loss           | 13.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 442          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063839033 |\n",
      "|    clip_fraction        | 0.00176      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0016       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.323        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -3.39e-05    |\n",
      "|    value_loss           | 9.26         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022340184 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.000569    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.446        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 2.15e-05     |\n",
      "|    value_loss           | 5.74         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 436         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004904094 |\n",
      "|    clip_fraction        | 0.00244     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000887   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.298       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000622   |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041971467 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00093      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.313        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 2.33         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 434         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005515515 |\n",
      "|    clip_fraction        | 0.00488     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.00177    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0838      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000231   |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011726716 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.00721     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    value_loss           | 0.898       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 432         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003753482 |\n",
      "|    clip_fraction        | 0.00767     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.00512     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 6.13e-05    |\n",
      "|    value_loss           | 0.543       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 431         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010276761 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.00443     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 0.357       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 430         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013073335 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.000708   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 430         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014181285 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.0107      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0269      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 429          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045562927 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.965       |\n",
      "|    explained_variance   | 0.011        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0178       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 0.0982       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 429         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005898294 |\n",
      "|    clip_fraction        | 0.0177      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.009       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0071      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 0.0641      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 428         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009221986 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 0.0407      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 428         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473115 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.771      |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    value_loss           | 0.0286      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 427         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008440014 |\n",
      "|    clip_fraction        | 0.0824      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00118     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    value_loss           | 0.0192      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2230c01edf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000) # Train model 100.000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88f0c",
   "metadata": {},
   "source": [
    "## 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5c1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models MountainCar', 'PPO_Model_MountainCar') # Locate path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path) #save model in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model #Delete model to simulate reloading in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e918fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env = env) # Loading again the model saved in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e372ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\PPO_Model_MountainCar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e3ff9",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f08a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfzor\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571bbd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d58ad",
   "metadata": {},
   "source": [
    "## 6. Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8ec15aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _states = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a85edd",
   "metadata": {},
   "source": [
    "## 7. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06431c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1') # Locating PPO_1 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b0e0498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\MountainCar\\\\PPO_1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e054646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76132e1e",
   "metadata": {},
   "source": [
    "#### Execute in command line the tensorboard visualization http://localhost:6006 stop the cell to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930f33e",
   "metadata": {},
   "source": [
    "## 8. Adding a callback to the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafbd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11c3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models MountainCar') #Where the best model is going to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c10c27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1) #Stop our training when we achieved a 200 rwd\n",
    "eval_callback = EvalCallback(env,  #callback that is triggered after each training run\n",
    "                            callback_on_new_best=stop_callback, #callback to run in the new best model\n",
    "                            eval_freq=10000, #Evaluation Frequency to 10.000 time steps\n",
    "                            best_model_save_path=save_path, # Save the model everytime there is a new best model\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d655a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6d390b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 874  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 570         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009148173 |\n",
      "|    clip_fraction        | 0.00425     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.000311   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 132         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017175258 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.0223      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000312    |\n",
      "|    value_loss           | 89.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062754606 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.00647     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 84.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029585888 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.00208      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000786    |\n",
      "|    value_loss           | 69.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 451   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051587226 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0.00276       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.02          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 2.32e-07      |\n",
      "|    value_loss           | 54.4          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 442           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028986737 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.00299      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.59          |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 8.62e-05      |\n",
      "|    value_loss           | 41.6          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009552193 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.00233     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.94        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 30.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 437         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008574183 |\n",
      "|    clip_fraction        | 0.00879     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.000315   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000404   |\n",
      "|    value_loss           | 21.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004345322 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 4.71e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.814       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 427   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 427          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020665599 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00207      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.19         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.000653     |\n",
      "|    value_loss           | 9.1          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006963927 |\n",
      "|    clip_fraction        | 0.00132     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.0024     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.466       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000234   |\n",
      "|    value_loss           | 5.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 425         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011173977 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 2.53e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0989      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006179574 |\n",
      "|    clip_fraction        | 0.00498     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.00134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.182       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.000783   |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009036274 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.00159    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.235       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 420   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004808857 |\n",
      "|    clip_fraction        | 0.0064      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.00171    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000532   |\n",
      "|    value_loss           | 0.984       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035205572 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.000674    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0893       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000117    |\n",
      "|    value_loss           | 0.575        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 420         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010713785 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.000145    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    value_loss           | 0.4         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 420         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007346098 |\n",
      "|    clip_fraction        | 0.00493     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00161     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    value_loss           | 0.249       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011795345 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.000738   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 416   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 416         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009546503 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | -0.000465   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 416          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076841274 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.923       |\n",
      "|    explained_variance   | -0.00126     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00321      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 9.48e-05     |\n",
      "|    value_loss           | 0.0675       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 416         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004655008 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.9        |\n",
      "|    explained_variance   | -0.00108    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.0431      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031776903 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.869       |\n",
      "|    explained_variance   | -0.00449     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00324      |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000941    |\n",
      "|    value_loss           | 0.0293       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010936242 |\n",
      "|    clip_fraction        | 0.0504      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.832      |\n",
      "|    explained_variance   | -0.000916   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    value_loss           | 0.0204      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2231b5a1ca0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) # Training model with callback argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c853d34",
   "metadata": {},
   "source": [
    "## 9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b89ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])] #dictionary neural network for our custom actor=PI and valueFunctn\n",
    "                                                              #128 un/eachLayer (4Lyrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d94ba72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#associating this new_Arch to the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e6ac2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 613  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 364          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084432345 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.000114    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.83         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 323           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033961056 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.08         |\n",
      "|    explained_variance   | 0.000804      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.92          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 7.98e-05      |\n",
      "|    value_loss           | 66.8          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 304         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011232878 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.000329    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.87        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 41.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8800, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8800         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027564755 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000104     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.42         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.000921     |\n",
      "|    value_loss           | 27.8         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 285   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002883919 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.000125    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.6         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.000449    |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009524411 |\n",
      "|    clip_fraction        | 0.00889     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -7.07e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000215   |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028264844 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.000154     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.458        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 0.000568     |\n",
      "|    value_loss           | 7.32         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034473375 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -0.00026     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.424        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.000248     |\n",
      "|    value_loss           | 4.47         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18800, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008954687 |\n",
      "|    clip_fraction        | 0.00742     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.000113    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.369       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.000269   |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 268   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015206605 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -1.41e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.342       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071594133 |\n",
      "|    clip_fraction        | 0.00811      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.000114    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.165        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000224    |\n",
      "|    value_loss           | 1.18         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100797 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -1.65e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    value_loss           | 0.757       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035485665 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 3.69e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0904       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000816    |\n",
      "|    value_loss           | 0.487        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28800, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28800        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034280987 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 1.23e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0387       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.000731     |\n",
      "|    value_loss           | 0.306        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 263   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012695992 |\n",
      "|    clip_fraction        | 0.00854     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -9.82e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00068     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000918   |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005079136 |\n",
      "|    clip_fraction        | 0.00303     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.0001     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000376   |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00983536 |\n",
      "|    clip_fraction        | 0.0689     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | -4.26e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00417   |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.00426   |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=38800, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38800        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067900973 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -0.000133    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0122       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 0.0619       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 261   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 38912 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003882326 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 3.4e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00826     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 0.0444      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044656796 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | -9.54e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0178       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    value_loss           | 0.0327       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009844003 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.000211   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 0.0199      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007042798 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.000371   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 0.0145      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48800, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010934975 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 1.34e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 0.00972     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 259   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 189   |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 259          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 197          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051224953 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.00121      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00691     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 0.00682      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2231512fac0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37dd0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with new architecture\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs) #using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "162d46ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f6006",
   "metadata": {},
   "source": [
    "## 10. Using DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99dc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN # DQN RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0a7c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14f211ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 2914     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 3305     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 3534     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 3571     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 3636     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.088    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 3670     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 4800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 3726     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 5600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 3767     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 6400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 3787     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 7200     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 2955     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 3027     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 8800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 3097     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 9600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 3149     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 10400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 3199     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 11200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 3235     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 3264     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 12800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 3304     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 13600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 3331     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 14400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 3363     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 15200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 3394     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 16000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 3418     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 16800    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 3077     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 17600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 3111     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 18400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 3137     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 19200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 3160     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 3192     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 20800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 3212     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 21600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 3229     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 22400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 3251     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 23200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 3287     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 3304     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 24800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 3325     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 3342     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 26400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 3356     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 27200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 3142     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 3159     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 28800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 3179     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 29600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 3194     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 30400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 3211     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 31200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 3228     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 3243     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 32800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 3259     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 33600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 3270     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 34400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 3285     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 35200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 3292     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 3305     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 36800    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 3149     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 37600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 3164     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 38400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 3178     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 39200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 3191     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 3203     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 40800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 3217     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 41600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 3222     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 42400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 3236     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 43200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 3247     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 44000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 3259     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 44800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 3268     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 45600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 3277     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 46400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 3288     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 47200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 3168     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 48000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 3178     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 48800    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 3189     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 49600    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x2231b637970>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) #50.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01367999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DQN_Path = os.path.join('Training','Saved Models MountainCar', 'DQN_Model_MountainCar')\n",
    "model.save(DQN_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28dbf245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\DQN_Model_MountainCar'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e99b3835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d6c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6294bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x2231b637970>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f589c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b9b7def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a270150-b587-4caf-a356-8ba0089ea3b6",
   "metadata": {},
   "source": [
    "## 11. Using A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b46e926c-b634-427f-b689-92923940f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C # A2C RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d54f5866-782b-4d8c-850b-9d5e05b28f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43f937ec-bbef-40ea-a272-a46af72d4af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\A2C_3\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 421      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.018   |\n",
      "|    explained_variance | -0.191   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | 1.62e-06 |\n",
      "|    value_loss         | 5.92e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0243  |\n",
      "|    explained_variance | 0.025    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | 2.38e-06 |\n",
      "|    value_loss         | 6.36e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 398      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0207  |\n",
      "|    explained_variance | -0.119   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | 2.22e-06 |\n",
      "|    value_loss         | 8.24e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0437  |\n",
      "|    explained_variance | -0.559   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | 4.13e-06 |\n",
      "|    value_loss         | 5.04e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0389  |\n",
      "|    explained_variance | -0.0729  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | 3.46e-06 |\n",
      "|    value_loss         | 4.43e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.042   |\n",
      "|    explained_variance | -0.319   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | 2.63e-06 |\n",
      "|    value_loss         | 2.22e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | -0.176   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | 2.5e-06  |\n",
      "|    value_loss         | 4.36e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0412  |\n",
      "|    explained_variance | -0.217   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | 3.87e-06 |\n",
      "|    value_loss         | 4.96e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0243  |\n",
      "|    explained_variance | -0.0316  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | 3.5e-06  |\n",
      "|    value_loss         | 1.36e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0418  |\n",
      "|    explained_variance | -0.507   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | 4.6e-06  |\n",
      "|    value_loss         | 6.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 398      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0238  |\n",
      "|    explained_variance | -0.19    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | 1.98e-06 |\n",
      "|    value_loss         | 4.68e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0373  |\n",
      "|    explained_variance | 0.0263   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | 3.65e-06 |\n",
      "|    value_loss         | 5.47e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0285  |\n",
      "|    explained_variance | -0.203   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 1.61e-06 |\n",
      "|    value_loss         | 2.04e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 399      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0477  |\n",
      "|    explained_variance | 0.104    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | 3.25e-06 |\n",
      "|    value_loss         | 2.4e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 400      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0187  |\n",
      "|    explained_variance | -0.217   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | 1.61e-06 |\n",
      "|    value_loss         | 5.56e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0363  |\n",
      "|    explained_variance | -0.361   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21519    |\n",
      "|    policy_loss        | 2.41e-06 |\n",
      "|    value_loss         | 2.64e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0284  |\n",
      "|    explained_variance | -0.252   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | 1.15e-06 |\n",
      "|    value_loss         | 1.1e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 384      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0233  |\n",
      "|    explained_variance | -0.0975  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | 1.91e-06 |\n",
      "|    value_loss         | 4.57e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 384      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0355  |\n",
      "|    explained_variance | -0.582   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | 2.42e-06 |\n",
      "|    value_loss         | 2.79e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 385      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.02    |\n",
      "|    explained_variance | -0.185   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | 1.42e-06 |\n",
      "|    value_loss         | 3.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 387      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.03    |\n",
      "|    explained_variance | 0.0502   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | 2.17e-06 |\n",
      "|    value_loss         | 3.33e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0206  |\n",
      "|    explained_variance | -0.199   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | 1.62e-06 |\n",
      "|    value_loss         | 4.35e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.03    |\n",
      "|    explained_variance | -84.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | 1.22e-06 |\n",
      "|    value_loss         | 1.16e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0161  |\n",
      "|    explained_variance | -0.0476  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22299    |\n",
      "|    policy_loss        | 1.68e-06 |\n",
      "|    value_loss         | 8.18e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0174  |\n",
      "|    explained_variance | 0.0151   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | 1.43e-06 |\n",
      "|    value_loss         | 4.72e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.014   |\n",
      "|    explained_variance | -0.178   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | 9.63e-07 |\n",
      "|    value_loss         | 3.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0196  |\n",
      "|    explained_variance | 4.33e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | 1.28e-06 |\n",
      "|    value_loss         | 2.94e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0186  |\n",
      "|    explained_variance | -0.0924  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | 1.67e-06 |\n",
      "|    value_loss         | 5.69e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0252  |\n",
      "|    explained_variance | 0.0394   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22799    |\n",
      "|    policy_loss        | 2.73e-06 |\n",
      "|    value_loss         | 7.56e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0183  |\n",
      "|    explained_variance | -0.205   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | 1.24e-06 |\n",
      "|    value_loss         | 3.28e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.034   |\n",
      "|    explained_variance | -2.38    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | 1.64e-06 |\n",
      "|    value_loss         | 1.53e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0219  |\n",
      "|    explained_variance | -0.24    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23099    |\n",
      "|    policy_loss        | 1.64e-06 |\n",
      "|    value_loss         | 3.87e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0318  |\n",
      "|    explained_variance | -1.22    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | 1.27e-06 |\n",
      "|    value_loss         | 1.03e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0142  |\n",
      "|    explained_variance | -0.226   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | 9.33e-07 |\n",
      "|    value_loss         | 3.55e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.018   |\n",
      "|    explained_variance | 0.0894   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | 9.1e-07  |\n",
      "|    value_loss         | 1.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 395      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0127  |\n",
      "|    explained_variance | -0.142   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 1.11e-06 |\n",
      "|    value_loss         | 6.51e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=17600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0275  |\n",
      "|    explained_variance | -1.96    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23519    |\n",
      "|    policy_loss        | 1.53e-06 |\n",
      "|    value_loss         | 2.18e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0266  |\n",
      "|    explained_variance | -0.164   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | 1.06e-06 |\n",
      "|    value_loss         | 1.02e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0141  |\n",
      "|    explained_variance | -0.175   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | 9.56e-07 |\n",
      "|    value_loss         | 3.69e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0251  |\n",
      "|    explained_variance | -1.54    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | 1.04e-06 |\n",
      "|    value_loss         | 1.26e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | -0.0358  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | 1.3e-06  |\n",
      "|    value_loss         | 5.41e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0213  |\n",
      "|    explained_variance | 0.0675   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | 1.28e-06 |\n",
      "|    value_loss         | 2.42e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0141  |\n",
      "|    explained_variance | -0.228   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | 8.41e-07 |\n",
      "|    value_loss         | 2.86e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0193  |\n",
      "|    explained_variance | 0.141    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | 7.09e-07 |\n",
      "|    value_loss         | 1.05e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0107  |\n",
      "|    explained_variance | -0.153   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | 7.46e-07 |\n",
      "|    value_loss         | 4.26e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0222  |\n",
      "|    explained_variance | 0.0459   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | 1.43e-06 |\n",
      "|    value_loss         | 2.77e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0169  |\n",
      "|    explained_variance | -0.188   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | 1.22e-06 |\n",
      "|    value_loss         | 4.02e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0273  |\n",
      "|    explained_variance | 0.0634   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24599    |\n",
      "|    policy_loss        | 1.56e-06 |\n",
      "|    value_loss         | 2.07e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.028   |\n",
      "|    explained_variance | -0.0626  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24699    |\n",
      "|    policy_loss        | 2.69e-06 |\n",
      "|    value_loss         | 5.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0629  |\n",
      "|    explained_variance | -0.561   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24799    |\n",
      "|    policy_loss        | 4.83e-06 |\n",
      "|    value_loss         | 2.93e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0203  |\n",
      "|    explained_variance | -0.106   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | 2.12e-06 |\n",
      "|    value_loss         | 8e-07    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0225  |\n",
      "|    explained_variance | 2.35e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | 2.04e-06 |\n",
      "|    value_loss         | 5.67e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0256  |\n",
      "|    explained_variance | -0.174   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25099    |\n",
      "|    policy_loss        | 2.28e-06 |\n",
      "|    value_loss         | 5.28e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0447  |\n",
      "|    explained_variance | 0.0132   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | 4.56e-06 |\n",
      "|    value_loss         | 5.77e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0284  |\n",
      "|    explained_variance | -0.0893  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | 2.42e-06 |\n",
      "|    value_loss         | 4.6e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0553  |\n",
      "|    explained_variance | -0.0498  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 4.9e-06  |\n",
      "|    value_loss         | 4.04e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 394      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0419  |\n",
      "|    explained_variance | -0.12    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25499    |\n",
      "|    policy_loss        | 4.44e-06 |\n",
      "|    value_loss         | 6.52e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=27600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0733  |\n",
      "|    explained_variance | -0.208   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25519    |\n",
      "|    policy_loss        | 5.33e-06 |\n",
      "|    value_loss         | 2.52e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0443  |\n",
      "|    explained_variance | 0.103    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25599    |\n",
      "|    policy_loss        | 4.38e-06 |\n",
      "|    value_loss         | 5.25e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0248  |\n",
      "|    explained_variance | -0.147   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | 2.3e-06  |\n",
      "|    value_loss         | 6.03e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 389      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.043   |\n",
      "|    explained_variance | -1.29    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | 2.98e-06 |\n",
      "|    value_loss         | 2.81e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0201  |\n",
      "|    explained_variance | -0.151   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | 1.94e-06 |\n",
      "|    value_loss         | 6.92e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0248  |\n",
      "|    explained_variance | 0.0249   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25999    |\n",
      "|    policy_loss        | 1.25e-06 |\n",
      "|    value_loss         | 1.69e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.017   |\n",
      "|    explained_variance | -0.128   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | 1.68e-06 |\n",
      "|    value_loss         | 7.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0358  |\n",
      "|    explained_variance | -0.456   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26199    |\n",
      "|    policy_loss        | 2.08e-06 |\n",
      "|    value_loss         | 2.1e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0266  |\n",
      "|    explained_variance | -0.135   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | 1.88e-06 |\n",
      "|    value_loss         | 3.32e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0577  |\n",
      "|    explained_variance | -0.111   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26399    |\n",
      "|    policy_loss        | 3.74e-06 |\n",
      "|    value_loss         | 2.19e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0378  |\n",
      "|    explained_variance | -0.0597  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | 2.63e-06 |\n",
      "|    value_loss         | 2.78e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0418  |\n",
      "|    explained_variance | 3.34e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | 3.49e-06 |\n",
      "|    value_loss         | 3.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 85       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0361  |\n",
      "|    explained_variance | -0.176   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | 3.08e-06 |\n",
      "|    value_loss         | 4.4e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0451  |\n",
      "|    explained_variance | 0.0355   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | 4.27e-06 |\n",
      "|    value_loss         | 4.79e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0392  |\n",
      "|    explained_variance | -0.149   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | 4.88e-06 |\n",
      "|    value_loss         | 9.25e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.0769   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | 3.45e-06 |\n",
      "|    value_loss         | 3.72e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0491  |\n",
      "|    explained_variance | -0.143   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | 4.33e-06 |\n",
      "|    value_loss         | 4.06e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0915  |\n",
      "|    explained_variance | -0.389   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | 7.83e-06 |\n",
      "|    value_loss         | 3.09e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0447  |\n",
      "|    explained_variance | -0.0893  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | 6.34e-06 |\n",
      "|    value_loss         | 1.1e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0595  |\n",
      "|    explained_variance | 0.0276   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | 6.18e-06 |\n",
      "|    value_loss         | 5.2e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0369  |\n",
      "|    explained_variance | -0.0951  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | 2.78e-06 |\n",
      "|    value_loss         | 3.23e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=37600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0427  |\n",
      "|    explained_variance | 0.0323   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27519    |\n",
      "|    policy_loss        | 3.54e-06 |\n",
      "|    value_loss         | 3.74e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0608  |\n",
      "|    explained_variance | 0.0476   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | 4.2e-06  |\n",
      "|    value_loss         | 2.3e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0515  |\n",
      "|    explained_variance | -0.0747  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27699    |\n",
      "|    policy_loss        | 5.2e-06  |\n",
      "|    value_loss         | 5.12e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0885  |\n",
      "|    explained_variance | -0.726   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | 6.09e-06 |\n",
      "|    value_loss         | 2.02e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0397  |\n",
      "|    explained_variance | -0.0955  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27899    |\n",
      "|    policy_loss        | 4.49e-06 |\n",
      "|    value_loss         | 7.38e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0509  |\n",
      "|    explained_variance | 0.0433   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27999    |\n",
      "|    policy_loss        | 5.64e-06 |\n",
      "|    value_loss         | 6.28e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0388  |\n",
      "|    explained_variance | -0.121   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | 2.59e-06 |\n",
      "|    value_loss         | 2.57e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.041   |\n",
      "|    explained_variance | -0.126   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | 2.22e-06 |\n",
      "|    value_loss         | 1.66e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0287  |\n",
      "|    explained_variance | -0.153   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28299    |\n",
      "|    policy_loss        | 2.35e-06 |\n",
      "|    value_loss         | 4.23e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0269  |\n",
      "|    explained_variance | 0.0269   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28399    |\n",
      "|    policy_loss        | 2.46e-06 |\n",
      "|    value_loss         | 5.28e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 391      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0181  |\n",
      "|    explained_variance | -0.146   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | 1.41e-06 |\n",
      "|    value_loss         | 4.62e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0427  |\n",
      "|    explained_variance | -3.61    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | 2.46e-06 |\n",
      "|    value_loss         | 1.98e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0365  |\n",
      "|    explained_variance | -0.0982  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | 4e-06    |\n",
      "|    value_loss         | 7.15e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 112      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0552  |\n",
      "|    explained_variance | 0.000934 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 3.7e-06  |\n",
      "|    value_loss         | 2.19e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 113      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0404  |\n",
      "|    explained_variance | -0.205   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28899    |\n",
      "|    policy_loss        | 3.05e-06 |\n",
      "|    value_loss         | 3.41e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0913  |\n",
      "|    explained_variance | -2.62    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | 3.61e-06 |\n",
      "|    value_loss         | 7.58e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 392      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0479  |\n",
      "|    explained_variance | -0.102   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | 6.01e-06 |\n",
      "|    value_loss         | 8.56e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0517  |\n",
      "|    explained_variance | 0.0357   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | 4.05e-06 |\n",
      "|    value_loss         | 3.15e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0416  |\n",
      "|    explained_variance | -0.131   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | 3.83e-06 |\n",
      "|    value_loss         | 4.76e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 119      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.039   |\n",
      "|    explained_variance | 0.0643   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | 3.12e-06 |\n",
      "|    value_loss         | 3.63e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 393      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0283  |\n",
      "|    explained_variance | -0.0554  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | 2.7e-06  |\n",
      "|    value_loss         | 5.9e-07  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=47600, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0375  |\n",
      "|    explained_variance | 0.0458   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29519    |\n",
      "|    policy_loss        | 2.64e-06 |\n",
      "|    value_loss         | 2.83e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0405  |\n",
      "|    explained_variance | -0.106   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | 3.74e-06 |\n",
      "|    value_loss         | 4.86e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0241  |\n",
      "|    explained_variance | -0.0916  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | 2.44e-06 |\n",
      "|    value_loss         | 7.06e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0237  |\n",
      "|    explained_variance | 0.000329 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | 2.32e-06 |\n",
      "|    value_loss         | 6.26e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0192  |\n",
      "|    explained_variance | -0.124   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | 1.43e-06 |\n",
      "|    value_loss         | 4.08e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0374  |\n",
      "|    explained_variance | -4.37    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | 2.1e-06  |\n",
      "|    value_loss         | 1.99e-07 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x2231b669a00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) #First 20.000 then 200.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bec4b96-81ff-45ec-95c8-8c9329f46ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_Path = os.path.join('Training','Saved Models MountainCar', 'A2C_Model_MountainCar')\n",
    "model.save(A2C_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0ba0d83-bd05-431a-b036-22bffc5966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\A2C_Model_MountainCar'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2C_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38fa67cd-e9bf-4b59-a95e-33fb40cdd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b66dc9f-d6c0-42a9-af7d-e44ccb4e9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49f38710-030c-47c0-8076-8a91252bf2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x2231b669a00>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b9bc2cf-8a3d-4d12-bd1b-74b4df3da29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing A2C Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9fdfcc1-8037-4ba5-b09b-d945d2c0b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552eb145-6103-449a-983e-fa83f2dbe640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
