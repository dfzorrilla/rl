{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6575b107",
   "metadata": {},
   "source": [
    "# TFM | Reinforcement Learning | Daniel Zorrilla | MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0745f64",
   "metadata": {},
   "source": [
    "## Installing additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aee1d",
   "metadata": {},
   "source": [
    "###### Installing stable baselines and pyglet library for developing games and other visually-rich applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561e7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58854c-77f5-4004-b9e8-a905ef61a7bc",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c101b-20bf-4dfe-9993-23ae40b186c3",
   "metadata": {},
   "source": [
    "## 1. Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6267c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # provides a way of using OS dependent functionality. (files)\n",
    "import gym # Open AI gym\n",
    "from stable_baselines3 import PPO #PPO RL Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Creates a simple vectorized wrapper for multiple environments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # Test how well a model is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3670b76",
   "metadata": {},
   "source": [
    "## 2. Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1625fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'MountainCar-v0' # Naming the MountainCar environment\n",
    "env = gym.make(environment_name) # Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-200.0\n",
      "Episode:2 Score:-200.0\n",
      "Episode:3 Score:-200.0\n",
      "Episode:4 Score:-200.0\n",
      "Episode:5 Score:-200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5  # Number of episodes\n",
    "for episode in range (1, episodes+1): # Resetting environment  \n",
    "    state = env.reset() \n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done: # While episode active\n",
    "        env.render() # Visualizing environment\n",
    "        action = env.action_space.sample() # Creating sample actions\n",
    "        n_state, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward # Getting score\n",
    "    print('Episode:{} Score:{}'.format(episode,score)) # Printing episode and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c331912",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # Closing the render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c237420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # Understanding the action space of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d11c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Action random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92fef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # Understanding the observation space of this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db8c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20501152, -0.05156683], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7efa8e",
   "metadata": {},
   "source": [
    "## 3. Train and create RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b6c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'MountainCar') #where it is saved the tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e57a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\MountainCar'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e153fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install Pytorch *conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffe8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name) # Create environment\n",
    "env = DummyVecEnv([lambda: env]) # Wrapped environment using DummyVecEnv\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path) # Creating PPO Algorithm with MultiLayerPerceptron Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6611270",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 538  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 693          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047560446 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.000513    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 134          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 780          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028242317 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00945      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000262    |\n",
      "|    value_loss           | 89.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008493632 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.00711     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    value_loss           | 84.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 865          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017090135 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00265     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00011     |\n",
      "|    value_loss           | 69.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 895         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004508563 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0026      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.39        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.000708   |\n",
      "|    value_loss           | 54.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 918         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009905254 |\n",
      "|    clip_fraction        | 0.00156     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00757     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.43        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    value_loss           | 41.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 932         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008742744 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00243     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.88        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 943         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014872023 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0074      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000628   |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 955         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003960885 |\n",
      "|    clip_fraction        | 0.00376     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.00365     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.623       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.000418    |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 967         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011507643 |\n",
      "|    clip_fraction        | 0.0188      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.00168     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    value_loss           | 9.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 966         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834241 |\n",
      "|    clip_fraction        | 0.0209      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.00114     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.395       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000363   |\n",
      "|    value_loss           | 5.83        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 977         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013675751 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | 0.00256     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 984         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002054053 |\n",
      "|    clip_fraction        | 0.00425     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.857      |\n",
      "|    explained_variance   | 0.000237    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.000396    |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 988          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039121606 |\n",
      "|    clip_fraction        | 0.00815      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.829       |\n",
      "|    explained_variance   | 0.00192      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000166    |\n",
      "|    value_loss           | 1.51         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 990          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055141337 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.82        |\n",
      "|    explained_variance   | 0.00243      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -9.1e-05     |\n",
      "|    value_loss           | 0.957        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 994          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037124113 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | 0.00399      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000782    |\n",
      "|    value_loss           | 0.591        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 998          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031741613 |\n",
      "|    clip_fraction        | 0.00391      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.763       |\n",
      "|    explained_variance   | 0.0023       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0406       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | 0.000366     |\n",
      "|    value_loss           | 0.372        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1001        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003980343 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.785      |\n",
      "|    explained_variance   | 0.00924     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000442   |\n",
      "|    value_loss           | 0.246       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1005         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062227305 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.846       |\n",
      "|    explained_variance   | 0.00935      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0224       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23e5ce84550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000) # Train model 100.000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88f0c",
   "metadata": {},
   "source": [
    "## 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5c1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models MountainCar', 'PPO_Model_MountainCar') # Locate path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path) #save model in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model #Delete model to simulate reloading in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e918fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env = env) # Loading again the model saved in PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e372ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\PPO_Model_MountainCar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e3ff9",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f08a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfzor\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571bbd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d58ad",
   "metadata": {},
   "source": [
    "## 6. Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ec15aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _states = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38aede1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a85edd",
   "metadata": {},
   "source": [
    "## 7. Viewing Logs in Tensorboard Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60825b-1394-4825-9b3e-2822271ace24",
   "metadata": {
    "tags": []
   },
   "source": [
    "!tensorboard dev upload --logdir {Path_To_Log} --name \"Experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76132e1e",
   "metadata": {},
   "source": [
    "#### Execute in command line the tensorboard visualization http://localhost:6006 stop the cell to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930f33e",
   "metadata": {},
   "source": [
    "## 8. Adding a callback to the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aafbd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c3b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models MountainCar') #Where the best model is going to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c10c27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1) #Stop our training when we achieved a 200 rwd\n",
    "eval_callback = EvalCallback(env,  #callback that is triggered after each training run\n",
    "                            callback_on_new_best=stop_callback, #callback to run in the new best model\n",
    "                            eval_freq=10000, #Evaluation Frequency to 10.000 time steps\n",
    "                            best_model_save_path=save_path, # Save the model everytime there is a new best model\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47d655a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d390b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2109 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1443        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010881687 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.000926    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1298        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008124093 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0367      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 89.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1244         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022730557 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | -0.00663     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000122    |\n",
      "|    value_loss           | 85.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058358302 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00301      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 70.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1132  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1124        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011978835 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.00185    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.36        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 55.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1113       |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01003431 |\n",
      "|    clip_fraction        | 0.00713    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.00064    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.54       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.000737  |\n",
      "|    value_loss           | 42.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1111        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013819486 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.000721    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.94        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1107        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015360626 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.00077    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.98        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    value_loss           | 21.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011870224 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00383     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1082  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1082        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008411305 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00308     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.704       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000864   |\n",
      "|    value_loss           | 9.26        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1082         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010922302 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | -0.000862    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.425        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 3.77e-06     |\n",
      "|    value_loss           | 6.12         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1072        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007966287 |\n",
      "|    clip_fraction        | 0.00405     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.000404   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.234       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000517   |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1072         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066428455 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 7.85e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.311        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000291    |\n",
      "|    value_loss           | 2.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127364835 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.000205     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 1.47         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1063  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1064         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0133306645 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.00031      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0768       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    value_loss           | 0.906        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1065        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005472936 |\n",
      "|    clip_fraction        | 0.00317     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00325     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0178      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000122   |\n",
      "|    value_loss           | 0.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1061        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010004316 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.97       |\n",
      "|    explained_variance   | -0.00196    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0977      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.000495   |\n",
      "|    value_loss           | 0.37        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1065        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012064962 |\n",
      "|    clip_fraction        | 0.0529      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.00116     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | -200       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00938691 |\n",
      "|    clip_fraction        | 0.0397     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.796     |\n",
      "|    explained_variance   | -0.000984  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0365     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00196   |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1055  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23e6b152fa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) # Training model with callback argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c853d34",
   "metadata": {},
   "source": [
    "## 9. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b89ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])] #dictionary neural network for our custom actor=PI and valueFunctn\n",
    "                                                              #128 un/eachLayer (4Lyrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d94ba72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#associating this new_Arch to the model\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e6ac2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1647 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 945          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028035373 |\n",
      "|    clip_fraction        | 0.0249       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.000134    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.01         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 34.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 829           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042193785 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | -0.0043       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.7           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000384     |\n",
      "|    value_loss           | 66            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019001343 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.000136     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.79         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -1.68e-05    |\n",
      "|    value_loss           | 40.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9040, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9040         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023709612 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.00034      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000363    |\n",
      "|    value_loss           | 27.6         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 720   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 708         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010311114 |\n",
      "|    clip_fraction        | 0.00654     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.000262   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.000465   |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 703         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012244609 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -7.33e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.648       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 696         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010676878 |\n",
      "|    clip_fraction        | 0.0108      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.000127    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.808       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -6.67e-05   |\n",
      "|    value_loss           | 6.77        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 695         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013832206 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -4.58e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.314       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00069    |\n",
      "|    value_loss           | 4.41        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19040, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004639952 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | -4.92e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.215       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 678   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 675        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00777129 |\n",
      "|    clip_fraction        | 0.013      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | -0.000101  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.379      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.000544  |\n",
      "|    value_loss           | 1.79       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 673         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008699162 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | -5.34e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000375   |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 672          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059743784 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.88        |\n",
      "|    explained_variance   | -7.75e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000731    |\n",
      "|    value_loss           | 0.732        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 673         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009903734 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | 2.28e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 0.493       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29040, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027564834 |\n",
      "|    clip_fraction        | 0.00532      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.793       |\n",
      "|    explained_variance   | -8.17e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.000423     |\n",
      "|    value_loss           | 0.319        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 664   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 664         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004509409 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | -1.23e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.069       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 663          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024639321 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.812       |\n",
      "|    explained_variance   | -0.000214    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 664         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009153964 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.743      |\n",
      "|    explained_variance   | -0.000227   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 664         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004230244 |\n",
      "|    clip_fraction        | 0.00586     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 1.37e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0057      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00011     |\n",
      "|    value_loss           | 0.0661      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39040, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056324694 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | -0.000101    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00474     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    value_loss           | 0.0424       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 659   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x23e6b08efa0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37dd0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with new architecture\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs) #using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "162d46ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f6006",
   "metadata": {},
   "source": [
    "## 10. Using DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99dc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN # DQN RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a7c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f211ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\DQN_3\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 8669     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 8593     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 8528     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 8868     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 8527     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 8611     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4800     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 8380     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5600     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 8548     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 8510     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 8538     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 6909     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 8880     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 6999     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 9680     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 7109     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 10480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 7115     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 11280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 7234     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 7293     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 7407     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 7425     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 7392     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 15280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 7113     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 16080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 7150     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 16880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 7010     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 17680    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 6298     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 18480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 6315     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 19280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 6326     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 20080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 6425     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 20880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 6483     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 21680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 6534     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 22480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 6566     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 23280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 6614     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 24080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 6679     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 24880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 6715     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 25680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 6776     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 26480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 6827     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 27280    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 6440     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 28080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 6495     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 28880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 6551     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 29680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 6598     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 30480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 6637     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 31280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 6693     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 32080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 6734     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 32880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 6780     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 33680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 6820     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 34480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 6842     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 35280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 6861     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 36080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 6884     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 36880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 6930     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 37680    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38080    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 6656     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 38480    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 6710     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 39280    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x23e6b183850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) #40.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01367999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DQN_Path = os.path.join('Training','Saved Models MountainCar', 'DQN_Model_MountainCar')\n",
    "model.save(DQN_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28dbf245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\DQN_Model_MountainCar'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e99b3835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d6c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6294bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x23e6b183850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f589c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing DQN Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b9b7def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a270150-b587-4caf-a356-8ba0089ea3b6",
   "metadata": {},
   "source": [
    "## 11. Using A2C Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b46e926c-b634-427f-b689-92923940f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C # A2C RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d54f5866-782b-4d8c-850b-9d5e05b28f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43f937ec-bbef-40ea-a272-a46af72d4af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\MountainCar\\A2C_3\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.451    |\n",
      "|    explained_variance | 6.91e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -0.000716 |\n",
      "|    value_loss         | 1.88e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 614       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.472    |\n",
      "|    explained_variance | -0.0675   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -0.000369 |\n",
      "|    value_loss         | 1.94e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 611       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.493    |\n",
      "|    explained_variance | 0.0495    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -0.000193 |\n",
      "|    value_loss         | 1.9e-06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.269   |\n",
      "|    explained_variance | -0.0988  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.00133 |\n",
      "|    value_loss         | 2.84e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 621       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0288   |\n",
      "|    explained_variance | 0.0272    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | -5.41e-06 |\n",
      "|    value_loss         | 2.21e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0809  |\n",
      "|    explained_variance | -0.014   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | -1.9e-05 |\n",
      "|    value_loss         | 2.2e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0769   |\n",
      "|    explained_variance | 0.0308    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -1.94e-05 |\n",
      "|    value_loss         | 2.62e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0892   |\n",
      "|    explained_variance | 0.00733   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -1.98e-05 |\n",
      "|    value_loss         | 1.9e-06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.175   |\n",
      "|    explained_variance | 0.0364   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | -5.3e-05 |\n",
      "|    value_loss         | 2.66e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.215    |\n",
      "|    explained_variance | 0.0299    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -7.19e-05 |\n",
      "|    value_loss         | 2.87e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.064    |\n",
      "|    explained_variance | 0.0303    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -1.51e-05 |\n",
      "|    value_loss         | 2.69e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 626       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.116    |\n",
      "|    explained_variance | 0.0541    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | -2.65e-05 |\n",
      "|    value_loss         | 1.88e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 620       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.154    |\n",
      "|    explained_variance | 0.0635    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -3.71e-05 |\n",
      "|    value_loss         | 1.91e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 617       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.114    |\n",
      "|    explained_variance | 0.0074    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -2.55e-05 |\n",
      "|    value_loss         | 1.85e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 617       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.208    |\n",
      "|    explained_variance | 0.012     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -5.05e-05 |\n",
      "|    value_loss         | 1.59e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 617       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.758    |\n",
      "|    explained_variance | 0.0596    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | -0.000691 |\n",
      "|    value_loss         | 2.16e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=8080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 200       |\n",
      "|    mean_reward        | -200      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 8080      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.568    |\n",
      "|    explained_variance | 2.12e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17615     |\n",
      "|    policy_loss        | -0.000265 |\n",
      "|    value_loss         | 2.48e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 595      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | 0.0282   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -0.00102 |\n",
      "|    value_loss         | 2.55e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 597      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.199   |\n",
      "|    explained_variance | 0.0845   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -6.6e-05 |\n",
      "|    value_loss         | 2.95e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.151    |\n",
      "|    explained_variance | 0.0173    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -3.87e-05 |\n",
      "|    value_loss         | 2.11e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.173    |\n",
      "|    explained_variance | 0.000129  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -4.13e-05 |\n",
      "|    value_loss         | 1.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 602       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0316   |\n",
      "|    explained_variance | 0.0106    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -5.84e-06 |\n",
      "|    value_loss         | 2.06e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 602      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0252  |\n",
      "|    explained_variance | 0.07     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | -5e-06   |\n",
      "|    value_loss         | 2.53e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 603      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00934 |\n",
      "|    explained_variance | 0.025    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | -1.4e-06 |\n",
      "|    value_loss         | 1.98e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0322   |\n",
      "|    explained_variance | 4.59e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -7.02e-06 |\n",
      "|    value_loss         | 2.78e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0299   |\n",
      "|    explained_variance | 5.45e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -5.65e-06 |\n",
      "|    value_loss         | 2.16e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0183   |\n",
      "|    explained_variance | 0.0301    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -2.89e-06 |\n",
      "|    value_loss         | 1.75e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00696 |\n",
      "|    explained_variance | 0.0295   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | -1.1e-06 |\n",
      "|    value_loss         | 2.32e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00861  |\n",
      "|    explained_variance | 0.00915   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -1.54e-06 |\n",
      "|    value_loss         | 2.77e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00285  |\n",
      "|    explained_variance | 0.0301    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -3.47e-07 |\n",
      "|    value_loss         | 1.75e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 609       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00315  |\n",
      "|    explained_variance | 8.05e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -3.77e-07 |\n",
      "|    value_loss         | 1.63e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 610       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00282  |\n",
      "|    explained_variance | 0.0223    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -3.48e-07 |\n",
      "|    value_loss         | 1.82e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 611       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00371  |\n",
      "|    explained_variance | 0.0127    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | -5.75e-07 |\n",
      "|    value_loss         | 2.61e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 612       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00297  |\n",
      "|    explained_variance | 0.00689   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -4.12e-07 |\n",
      "|    value_loss         | 2.2e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 612       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00393  |\n",
      "|    explained_variance | 0.0661    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | -5.63e-07 |\n",
      "|    value_loss         | 2.23e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 613       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00301  |\n",
      "|    explained_variance | 7.09e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -3.76e-07 |\n",
      "|    value_loss         | 1.79e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 613       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00677  |\n",
      "|    explained_variance | 4.95e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | -1.16e-06 |\n",
      "|    value_loss         | 2.73e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=18080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | -200     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18080    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00676 |\n",
      "|    explained_variance | -0.00747 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19615    |\n",
      "|    policy_loss        | -9.7e-07 |\n",
      "|    value_loss         | 1.91e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 601       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0143   |\n",
      "|    explained_variance | 0.0292    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19699     |\n",
      "|    policy_loss        | -1.97e-06 |\n",
      "|    value_loss         | 1.44e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 603      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0184  |\n",
      "|    explained_variance | 0.0175   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | -3.2e-06 |\n",
      "|    value_loss         | 2.06e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0137   |\n",
      "|    explained_variance | 0.0296    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -2.11e-06 |\n",
      "|    value_loss         | 1.82e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 604       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0262   |\n",
      "|    explained_variance | 0.069     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -5.72e-06 |\n",
      "|    value_loss         | 2.9e-06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0419  |\n",
      "|    explained_variance | 0.0362   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | -8.1e-06 |\n",
      "|    value_loss         | 1.89e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0491   |\n",
      "|    explained_variance | 0.011     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20199     |\n",
      "|    policy_loss        | -9.99e-06 |\n",
      "|    value_loss         | 1.92e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 604       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0313   |\n",
      "|    explained_variance | 0.0185    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20299     |\n",
      "|    policy_loss        | -5.66e-06 |\n",
      "|    value_loss         | 1.84e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.031   |\n",
      "|    explained_variance | 0.108    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | -6e-06   |\n",
      "|    value_loss         | 2.09e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 604       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0223   |\n",
      "|    explained_variance | 0.039     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20499     |\n",
      "|    policy_loss        | -3.54e-06 |\n",
      "|    value_loss         | 1.6e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0195   |\n",
      "|    explained_variance | 0.0399    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20599     |\n",
      "|    policy_loss        | -3.61e-06 |\n",
      "|    value_loss         | 2.22e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00662  |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20699     |\n",
      "|    policy_loss        | -8.64e-07 |\n",
      "|    value_loss         | 1.55e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00956  |\n",
      "|    explained_variance | 2.05e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20799     |\n",
      "|    policy_loss        | -1.68e-06 |\n",
      "|    value_loss         | 2.56e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00776  |\n",
      "|    explained_variance | 0.0246    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20899     |\n",
      "|    policy_loss        | -1.17e-06 |\n",
      "|    value_loss         | 2.02e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00681  |\n",
      "|    explained_variance | 0.0207    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20999     |\n",
      "|    policy_loss        | -1.22e-06 |\n",
      "|    value_loss         | 2.91e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00561  |\n",
      "|    explained_variance | 0.0192    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21099     |\n",
      "|    policy_loss        | -7.48e-07 |\n",
      "|    value_loss         | 1.73e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00687  |\n",
      "|    explained_variance | 0.0197    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21199     |\n",
      "|    policy_loss        | -1.11e-06 |\n",
      "|    value_loss         | 2.35e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00557  |\n",
      "|    explained_variance | 0.0181    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21299     |\n",
      "|    policy_loss        | -7.77e-07 |\n",
      "|    value_loss         | 1.88e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 607      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00652 |\n",
      "|    explained_variance | 4.47e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | -1.1e-06 |\n",
      "|    value_loss         | 2.64e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00247  |\n",
      "|    explained_variance | 0.0287    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21499     |\n",
      "|    policy_loss        | -3.22e-07 |\n",
      "|    value_loss         | 1.97e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 609       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00365  |\n",
      "|    explained_variance | 0.163     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21599     |\n",
      "|    policy_loss        | -6.05e-07 |\n",
      "|    value_loss         | 2.88e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=28080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 200       |\n",
      "|    mean_reward        | -200      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28080     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00369  |\n",
      "|    explained_variance | 0.157     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21615     |\n",
      "|    policy_loss        | -5.05e-07 |\n",
      "|    value_loss         | 1.94e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00264  |\n",
      "|    explained_variance | 9.18e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21699     |\n",
      "|    policy_loss        | -2.98e-07 |\n",
      "|    value_loss         | 1.46e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 604      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00361 |\n",
      "|    explained_variance | 0.17     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -4.9e-07 |\n",
      "|    value_loss         | 1.95e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 604       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00241  |\n",
      "|    explained_variance | 0.035     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21899     |\n",
      "|    policy_loss        | -3.15e-07 |\n",
      "|    value_loss         | 2e-06     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 605      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00281 |\n",
      "|    explained_variance | 4.95e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | -4.4e-07 |\n",
      "|    value_loss         | 2.7e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00259  |\n",
      "|    explained_variance | 4.61e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22099     |\n",
      "|    policy_loss        | -3.87e-07 |\n",
      "|    value_loss         | 2.54e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00364  |\n",
      "|    explained_variance | 0.186     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22199     |\n",
      "|    policy_loss        | -5.48e-07 |\n",
      "|    value_loss         | 2.37e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00239  |\n",
      "|    explained_variance | 0.0591    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22299     |\n",
      "|    policy_loss        | -2.19e-07 |\n",
      "|    value_loss         | 9.81e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0031   |\n",
      "|    explained_variance | 0.0141    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22399     |\n",
      "|    policy_loss        | -4.29e-07 |\n",
      "|    value_loss         | 2.06e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00261  |\n",
      "|    explained_variance | 0.0172    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22499     |\n",
      "|    policy_loss        | -3.59e-07 |\n",
      "|    value_loss         | 2.15e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00343  |\n",
      "|    explained_variance | 0.0802    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22599     |\n",
      "|    policy_loss        | -5.22e-07 |\n",
      "|    value_loss         | 2.44e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00237  |\n",
      "|    explained_variance | 0.0465    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22699     |\n",
      "|    policy_loss        | -2.54e-07 |\n",
      "|    value_loss         | 1.35e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00338  |\n",
      "|    explained_variance | 0.0747    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22799     |\n",
      "|    policy_loss        | -5.57e-07 |\n",
      "|    value_loss         | 2.88e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00236  |\n",
      "|    explained_variance | 0.0315    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22899     |\n",
      "|    policy_loss        | -3.07e-07 |\n",
      "|    value_loss         | 2.01e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00178  |\n",
      "|    explained_variance | 0.0163    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22999     |\n",
      "|    policy_loss        | -3.02e-07 |\n",
      "|    value_loss         | 3.45e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00142  |\n",
      "|    explained_variance | 0.0485    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | -1.53e-07 |\n",
      "|    value_loss         | 1.49e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 608       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00197  |\n",
      "|    explained_variance | 0.0766    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23199     |\n",
      "|    policy_loss        | -2.41e-07 |\n",
      "|    value_loss         | 1.78e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 609       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0015   |\n",
      "|    explained_variance | 0.00689   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23299     |\n",
      "|    policy_loss        | -1.97e-07 |\n",
      "|    value_loss         | 2.19e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 609       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00183  |\n",
      "|    explained_variance | 0.0201    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23399     |\n",
      "|    policy_loss        | -2.94e-07 |\n",
      "|    value_loss         | 3.07e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 610       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00141  |\n",
      "|    explained_variance | 0.0399    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23499     |\n",
      "|    policy_loss        | -1.55e-07 |\n",
      "|    value_loss         | 1.55e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 610       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00199  |\n",
      "|    explained_variance | 0.101     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23599     |\n",
      "|    policy_loss        | -2.42e-07 |\n",
      "|    value_loss         | 1.78e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=38080, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 200       |\n",
      "|    mean_reward        | -200      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 38080     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.002    |\n",
      "|    explained_variance | 0.0404    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23615     |\n",
      "|    policy_loss        | -2.97e-07 |\n",
      "|    value_loss         | 2.59e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 606      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00139 |\n",
      "|    explained_variance | 0.0338   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | -1.6e-07 |\n",
      "|    value_loss         | 1.72e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00157  |\n",
      "|    explained_variance | 4.91e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23799     |\n",
      "|    policy_loss        | -2.18e-07 |\n",
      "|    value_loss         | 2.38e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00138  |\n",
      "|    explained_variance | 0.0194    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23899     |\n",
      "|    policy_loss        | -1.86e-07 |\n",
      "|    value_loss         | 2.38e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00189  |\n",
      "|    explained_variance | 0.0445    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23999     |\n",
      "|    policy_loss        | -2.71e-07 |\n",
      "|    value_loss         | 2.46e-06  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x23e6b18dee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, callback=eval_callback) #First 40.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3bec4b96-81ff-45ec-95c8-8c9329f46ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_Path = os.path.join('Training','Saved Models MountainCar', 'A2C_Model_MountainCar')\n",
    "model.save(A2C_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0ba0d83-bd05-431a-b036-22bffc5966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models MountainCar\\\\A2C_Model_MountainCar'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2C_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38fa67cd-e9bf-4b59-a95e-33fb40cdd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True) # Evaluating model with 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b66dc9f-d6c0-42a9-af7d-e44ccb4e9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49f38710-030c-47c0-8076-8a91252bf2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x23e6b18dee0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b9bc2cf-8a3d-4d12-bd1b-74b4df3da29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[-200.]\n",
      "Episode:2 Score:[-200.]\n",
      "Episode:3 Score:[-200.]\n",
      "Episode:4 Score:[-200.]\n",
      "Episode:5 Score:[-200.]\n"
     ]
    }
   ],
   "source": [
    "# Testing A2C Trained algorithm\n",
    "episodes = 5\n",
    "for episode in range (1, episodes+1): \n",
    "    obs = env.reset()  # Resetting episodes\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Visualize model\n",
    "        action, _ = model.predict(obs) # Using trained model to predict actions\n",
    "        obs, reward, done, info = env.step(action) # Defining step action\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9fdfcc1-8037-4ba5-b09b-d945d2c0b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
